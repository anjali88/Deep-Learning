{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Perceptron</h1>\n",
    "\n",
    "<h2>From BNNs to ANNs</h2>\n",
    "<p>Where do Artificial Neural Networks came from? As many other inventions, the concept behind them came from nature. In particular, <b>our brain</b>.</p>\n",
    "\n",
    "<h2><i>Biological Neurons</i></h2>\n",
    "<img src=\"img/neurons.png\" width=\"500\" height=\"500\"></img>\n",
    "<h2><i>Perceptron</i></h2>\n",
    "<img src=\"img/perceptron.png\" width=\"500\" height=\"500\"></img>\n",
    "\n",
    "<p>They are very similar, aren't they? We have <b>inputs</b>, some kind of <b>computation</b> and an <b>output</b>. The Perceptron is one of the simplest ANN architectures. It is based on a particular artificial neuron called <i>linear threshold unit</i> (LTU). \n",
    "<p><b>How it works?</b></p>\n",
    "<ul>\n",
    "    <li>inputs and outputs are numbers (not binary values)</li>\n",
    "    <li>each input connection is associated with a weight</li>\n",
    "    <li>the LTU computes a weighted sum of its inputs (z = <b>w'.x</b>)</li>\n",
    "    <li>then applies a step function (heaviside function or sign function) to that sum and outputs the result</li>\n",
    "</ul>\n",
    "\n",
    "<p><b>Training a Perceptron</b><p>\n",
    "<p>For each instance, it makes its predictions. For every output neuron that produced a wrong prediction, it reinforces the connection weights from the inputs that would have contributed to the correct prediction.</p>\n",
    "<img src=\"img/learning_rule.png\" width=\"300\" height=\"50\"></img>\n",
    "\n",
    "<p><b>What's the problem?</b><p>\n",
    "<p>Like any other linear classification model, the Perceptron cannot solve some trivial problems, like XOR classification problem.</p>\n",
    "\n",
    "<p><b>How to solve?</b><p>\n",
    "<p>If we stack multiple Perceptrons we can eliminate a lot of these limitations. So Multi-Layer Perceptron is born and from that all the ANNs we know today.</p>\n",
    "\n",
    "<img src=\"img/multi.jpg\" width=\"400\" height=\"400\"></img>\n",
    "\n",
    "<p>As you can see we have one or more layers of LTU, called <i>hidden layers</i> (if the hidden layers are two or more the ANN is called Deep Neural Network).</p>\n",
    "<p><b>What kind of activation functions here?</b></p>\n",
    "<p>As you can see from the image, in the hidden layers we use a non-linear function, while in the output layer we use a linear one. Why? </p>\n",
    "<p>That's because of the training algorithm we're gonna to use with a MLP: <i>backpropagation</i>.</p>\n",
    "<p>I'm not going to explain here how it works (reference <a href=\"https://en.wikipedia.org/wiki/Backpropagation\">HERE</a>), but in the final step we use the Gradient Descent step to reduce the error, and because of the step function contains only flat segment there is no gradient to work with.</p>\n",
    "\n",
    "<p><b>Common Activation Functions: </b></p>\n",
    "<ul>\n",
    "    <li>logistic function</li>\n",
    "    <li>hyperbolic tangent function</li>\n",
    "    <li>ReLU function</li>\n",
    "</ul>\n",
    "<p>Insted, the linear activation function which we use in the output layer is the <b>Softmax function</b>.</p>\n",
    "\n",
    "<h1>Some code now!</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)]\n",
    "y = (iris.target == 0).astype(np.int) # Iris Setosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "per_clf = Perceptron(random_state=42)\n",
    "per_clf.fit(X, y)\n",
    "\n",
    "y_pred = per_clf.predict([[2, 0.5]])\n",
    "print(y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
