{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "2G-Ip9WAfuDx"
   },
   "outputs": [],
   "source": [
    "#from google.colab import files\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 72,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15812,
     "status": "ok",
     "timestamp": 1526158278237,
     "user": {
      "displayName": "Mattia Mancassola",
      "photoUrl": "//lh3.googleusercontent.com/-lyRwyRCbSzY/AAAAAAAAAAI/AAAAAAAAUYQ/zHzRCLhMZRs/s50-c-k-no/photo.jpg",
      "userId": "116324510352588137280"
     },
     "user_tz": -120
    },
    "id": "_AZa1yy5gYck",
    "outputId": "d3d3842d-7629-4d42-b852-84426bd9443f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-6d4a5f67-6241-4d2b-a297-39685fa73f4e\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-6d4a5f67-6241-4d2b-a297-39685fa73f4e\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving nietzsche.txt to nietzsche.txt\n"
     ]
    }
   ],
   "source": [
    "'''This is for the Colab's notebook'''\n",
    "#uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 666,
     "status": "ok",
     "timestamp": 1526158279974,
     "user": {
      "displayName": "Mattia Mancassola",
      "photoUrl": "//lh3.googleusercontent.com/-lyRwyRCbSzY/AAAAAAAAAAI/AAAAAAAAUYQ/zHzRCLhMZRs/s50-c-k-no/photo.jpg",
      "userId": "116324510352588137280"
     },
     "user_tz": -120
    },
    "id": "e3hrryEFgvox",
    "outputId": "e46eae10-3e1f-4836-c53c-197e05299211"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35 81 66 79 32 36 66 46 46 46 31 18 35 35 47 31  8 73 33 24 22 57 78 22\n",
      " 24 50 67 71 22 57 24  0 14 24 78 24 77 10 27 78 62 44 44 77 57 78 22 24\n",
      " 22 57 15 62 23 24  8 14 24 22 57 15 67 15 24 62 10 22 24  9 67 10 71 62\n",
      " 45 46 75 10 67 24 14 71 14 21 15 34 22  0 62  9 24 22 57 78 22 24 78  2\n",
      "  2 24 21 57]\n"
     ]
    }
   ],
   "source": [
    "with open('nietzsche.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = set(text)\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "encoded = np.array([vocab_to_int[c] for c in text], dtype=np.int32)\n",
    "\n",
    "print(encoded[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "j3jFeVAMLAeZ"
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "\n",
    "    # Get the batch size and number of batches we can make\n",
    "    batch_size = n_seqs * n_steps \n",
    "    n_batches =  len(arr)//batch_size\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr =  arr[:n_batches*batch_size]\n",
    "    \n",
    "    # Reshape into n_seqs rows\n",
    "    arr = arr.reshape((n_seqs,-1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # The features\n",
    "        x = arr[:,n:n+n_steps]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros(x.shape)\n",
    "        y[:,:-1],y[:,-1] = x[:,1:] ,x[:,0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "hQjAWtgw6Qvp"
   },
   "outputs": [],
   "source": [
    "batch_size = 100        # Sequences per batch\n",
    "num_steps = 150         # Number of sequence steps per batch\n",
    "lstm_size = 512         # Size of hidden layers in LSTMs\n",
    "num_layers = 2          # Number of LSTM layers\n",
    "learning_rate = 0.001   # Learning rate\n",
    "keep_prob = 0.5         # Dropout keep probability\n",
    "epochs = 40\n",
    "save_every_n = 200      # Save every N iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "X6u2wRCjiuh0"
   },
   "outputs": [],
   "source": [
    "def build_inputs(batch_size, num_steps):\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name=\"inputs\")\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps], name=\"targets\")\n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "\n",
    "    return inputs, targets, keep_prob\n",
    "\n",
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n",
    "\n",
    "    def build_cell(lstm_size, keep_prob):\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        return drop\n",
    "\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size, keep_prob) for _ in range(num_layers)])\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    return cell, initial_state\n",
    "\n",
    "def build_output(lstm_output, in_size, out_size):\n",
    "    seq_output = tf.concat(lstm_output, axis=1)\n",
    "    x = tf.reshape(seq_output, [-1, in_size])\n",
    "\n",
    "    with tf.variable_scope(\"softmax\"):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((in_size, out_size),stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(out_size))\n",
    "\n",
    "    logits = tf.matmul(x,softmax_w) + softmax_b\n",
    "    out = tf.nn.softmax(logits, name='predictions')\n",
    "    return out, logits\n",
    "\n",
    "def build_loss(logits, targets, lstm_size, num_classes):\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "    y_reshaped = tf.reshape(y_one_hot, logits.get_shape())\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "class CharRNN:\n",
    "    def __init__(self, num_classes, batch_size, num_steps, lstm_size,\n",
    "                    num_layers, learning_rate,\n",
    "                    grad_clip=5, sampling=False):\n",
    "        \n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        # Build the input placeholder tensors\n",
    "        self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_steps)\n",
    "\n",
    "        # Build the LSTM cells\n",
    "        cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, keep_prob)\n",
    "\n",
    "        ## Run the data through RNN layers\n",
    "        # First one-hot encode the input tokens\n",
    "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "\n",
    "        # Run each sequence step through the RNN and collect the outputs\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=self.initial_state)\n",
    "        self.final_state = state\n",
    "\n",
    "        # Get softmax predictions and logits\n",
    "        self.prediction, self.logits = build_output(outputs, lstm_size, num_classes)\n",
    "\n",
    "        # Loss and Optimizer (with gradient clipping)\n",
    "        self.loss = build_loss(self.logits, self.targets, lstm_size, num_classes)\n",
    "        self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Ym5CJuc65aoI"
   },
   "outputs": [],
   "source": [
    "def train(encoded, vocab):\n",
    "    model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps,\n",
    "                    lstm_size=lstm_size, num_layers=num_layers,\n",
    "                    learning_rate=learning_rate)\n",
    "    \n",
    "    saver = tf.train.Saver(max_to_keep=100)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        # saver.restore(sess,\"checkpoints/______.ckpt\")\n",
    "        counter = 0\n",
    "        for e in range(epochs):\n",
    "            new_state = sess.run(model.initial_state)\n",
    "            loss = 0\n",
    "            for x, y in get_batches(encoded, batch_size, num_steps):\n",
    "                counter += 1\n",
    "                start = time.time()\n",
    "                feed = {model.inputs:x,\n",
    "                        model.targets:y,\n",
    "                        model.keep_prob:keep_prob,\n",
    "                        model.initial_state:new_state\n",
    "                        }\n",
    "                batch_loss, new_state, _ = sess.run([model.loss,\n",
    "                                                 model.final_state,\n",
    "                                                 model.optimizer],\n",
    "                                                 feed_dict=feed)\n",
    "                end = time.time()\n",
    "                print('Epoch: {}/{}... '.format(e+1, epochs),\n",
    "                      'Training Step: {}... '.format(counter),\n",
    "                      'Training loss: {:.4f}... '.format(batch_loss),\n",
    "                      '{:.4f} sec/batch'.format((end-start)))\n",
    "\n",
    "                if (counter % save_every_n == 0):\n",
    "                    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "\n",
    "        saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "oef7XguO5bdp"
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=3):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c\n",
    "\n",
    "def sample(checkpoint, vocab, vocab_to_int, int_to_vocab, n_samples, lstm_size, vocab_size, seed=\"The \"):\n",
    "    samples = [c for c in seed]\n",
    "    model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps,\n",
    "                    lstm_size=lstm_size, num_layers=num_layers,\n",
    "                    learning_rate=learning_rate, sampling=True)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in seed:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab_to_int[c]\n",
    "            feed = {model.inputs:x,\n",
    "                    model.keep_prob:1.0,\n",
    "                    model.initial_state:new_state\n",
    "                    }\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state],\n",
    "                                        feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0, 0] = c\n",
    "            feed = {model.inputs:x,\n",
    "                    model.keep_prob:1.0,\n",
    "                    model.initial_state:new_state\n",
    "                    }\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state],\n",
    "                                        feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "\n",
    "    return \"\".join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 27217
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 831104,
     "status": "ok",
     "timestamp": 1526164392930,
     "user": {
      "displayName": "Mattia Mancassola",
      "photoUrl": "//lh3.googleusercontent.com/-lyRwyRCbSzY/AAAAAAAAAAI/AAAAAAAAUYQ/zHzRCLhMZRs/s50-c-k-no/photo.jpg",
      "userId": "116324510352588137280"
     },
     "user_tz": -120
    },
    "id": "4JLkM6Vc6IuF",
    "outputId": "4cd1d46e-049b-4665-8c6c-94975c2e1869"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40...  Training Step: 1...  Training loss: 4.4329...  0.6840 sec/batch\n",
      "Epoch: 1/40...  Training Step: 2...  Training loss: 4.3549...  0.5096 sec/batch\n",
      "Epoch: 1/40...  Training Step: 3...  Training loss: 3.9355...  0.5030 sec/batch\n",
      "Epoch: 1/40...  Training Step: 4...  Training loss: 5.7739...  0.5170 sec/batch\n",
      "Epoch: 1/40...  Training Step: 5...  Training loss: 4.3575...  0.5108 sec/batch\n",
      "Epoch: 1/40...  Training Step: 6...  Training loss: 3.7824...  0.5223 sec/batch\n",
      "Epoch: 1/40...  Training Step: 7...  Training loss: 3.6650...  0.5045 sec/batch\n",
      "Epoch: 1/40...  Training Step: 8...  Training loss: 3.5711...  0.5055 sec/batch\n",
      "Epoch: 1/40...  Training Step: 9...  Training loss: 3.5029...  0.5109 sec/batch\n",
      "Epoch: 1/40...  Training Step: 10...  Training loss: 3.4793...  0.5175 sec/batch\n",
      "Epoch: 1/40...  Training Step: 11...  Training loss: 3.4260...  0.5234 sec/batch\n",
      "Epoch: 1/40...  Training Step: 12...  Training loss: 3.4061...  0.5135 sec/batch\n",
      "Epoch: 1/40...  Training Step: 13...  Training loss: 3.3599...  0.5183 sec/batch\n",
      "Epoch: 1/40...  Training Step: 14...  Training loss: 3.3461...  0.5066 sec/batch\n",
      "Epoch: 1/40...  Training Step: 15...  Training loss: 3.3536...  0.5181 sec/batch\n",
      "Epoch: 1/40...  Training Step: 16...  Training loss: 3.3170...  0.5054 sec/batch\n",
      "Epoch: 1/40...  Training Step: 17...  Training loss: 3.2972...  0.5020 sec/batch\n",
      "Epoch: 1/40...  Training Step: 18...  Training loss: 3.2943...  0.5223 sec/batch\n",
      "Epoch: 1/40...  Training Step: 19...  Training loss: 3.2986...  0.5162 sec/batch\n",
      "Epoch: 1/40...  Training Step: 20...  Training loss: 3.3130...  0.5078 sec/batch\n",
      "Epoch: 1/40...  Training Step: 21...  Training loss: 3.2929...  0.5116 sec/batch\n",
      "Epoch: 1/40...  Training Step: 22...  Training loss: 3.2970...  0.5232 sec/batch\n",
      "Epoch: 1/40...  Training Step: 23...  Training loss: 3.2705...  0.5115 sec/batch\n",
      "Epoch: 1/40...  Training Step: 24...  Training loss: 3.2529...  0.5166 sec/batch\n",
      "Epoch: 1/40...  Training Step: 25...  Training loss: 3.2317...  0.5152 sec/batch\n",
      "Epoch: 1/40...  Training Step: 26...  Training loss: 3.2243...  0.5141 sec/batch\n",
      "Epoch: 1/40...  Training Step: 27...  Training loss: 3.2628...  0.5101 sec/batch\n",
      "Epoch: 1/40...  Training Step: 28...  Training loss: 3.2125...  0.5169 sec/batch\n",
      "Epoch: 1/40...  Training Step: 29...  Training loss: 3.2300...  0.5060 sec/batch\n",
      "Epoch: 1/40...  Training Step: 30...  Training loss: 3.1938...  0.5063 sec/batch\n",
      "Epoch: 1/40...  Training Step: 31...  Training loss: 3.1961...  0.5028 sec/batch\n",
      "Epoch: 1/40...  Training Step: 32...  Training loss: 3.2036...  0.5181 sec/batch\n",
      "Epoch: 1/40...  Training Step: 33...  Training loss: 3.2058...  0.5162 sec/batch\n",
      "Epoch: 1/40...  Training Step: 34...  Training loss: 3.2138...  0.5149 sec/batch\n",
      "Epoch: 1/40...  Training Step: 35...  Training loss: 3.2005...  0.5181 sec/batch\n",
      "Epoch: 1/40...  Training Step: 36...  Training loss: 3.2106...  0.4994 sec/batch\n",
      "Epoch: 1/40...  Training Step: 37...  Training loss: 3.1966...  0.5033 sec/batch\n",
      "Epoch: 1/40...  Training Step: 38...  Training loss: 3.1949...  0.5188 sec/batch\n",
      "Epoch: 1/40...  Training Step: 39...  Training loss: 3.1874...  0.5154 sec/batch\n",
      "Epoch: 1/40...  Training Step: 40...  Training loss: 3.1945...  0.5253 sec/batch\n",
      "Epoch: 2/40...  Training Step: 41...  Training loss: 3.6193...  0.5161 sec/batch\n",
      "Epoch: 2/40...  Training Step: 42...  Training loss: 3.4982...  0.5126 sec/batch\n",
      "Epoch: 2/40...  Training Step: 43...  Training loss: 3.2122...  0.5202 sec/batch\n",
      "Epoch: 2/40...  Training Step: 44...  Training loss: 3.1667...  0.5010 sec/batch\n",
      "Epoch: 2/40...  Training Step: 45...  Training loss: 3.1874...  0.5113 sec/batch\n",
      "Epoch: 2/40...  Training Step: 46...  Training loss: 3.1851...  0.5187 sec/batch\n",
      "Epoch: 2/40...  Training Step: 47...  Training loss: 3.1864...  0.5202 sec/batch\n",
      "Epoch: 2/40...  Training Step: 48...  Training loss: 3.1657...  0.5086 sec/batch\n",
      "Epoch: 2/40...  Training Step: 49...  Training loss: 3.1660...  0.5124 sec/batch\n",
      "Epoch: 2/40...  Training Step: 50...  Training loss: 3.1817...  0.5109 sec/batch\n",
      "Epoch: 2/40...  Training Step: 51...  Training loss: 3.1774...  0.5099 sec/batch\n",
      "Epoch: 2/40...  Training Step: 52...  Training loss: 3.1960...  0.5062 sec/batch\n",
      "Epoch: 2/40...  Training Step: 53...  Training loss: 3.1661...  0.5169 sec/batch\n",
      "Epoch: 2/40...  Training Step: 54...  Training loss: 3.1789...  0.5060 sec/batch\n",
      "Epoch: 2/40...  Training Step: 55...  Training loss: 3.1924...  0.5146 sec/batch\n",
      "Epoch: 2/40...  Training Step: 56...  Training loss: 3.1730...  0.5163 sec/batch\n",
      "Epoch: 2/40...  Training Step: 57...  Training loss: 3.1647...  0.5105 sec/batch\n",
      "Epoch: 2/40...  Training Step: 58...  Training loss: 3.1654...  0.5102 sec/batch\n",
      "Epoch: 2/40...  Training Step: 59...  Training loss: 3.1773...  0.5103 sec/batch\n",
      "Epoch: 2/40...  Training Step: 60...  Training loss: 3.1870...  0.5119 sec/batch\n",
      "Epoch: 2/40...  Training Step: 61...  Training loss: 3.1731...  0.5092 sec/batch\n",
      "Epoch: 2/40...  Training Step: 62...  Training loss: 3.1732...  0.5113 sec/batch\n",
      "Epoch: 2/40...  Training Step: 63...  Training loss: 3.1701...  0.5232 sec/batch\n",
      "Epoch: 2/40...  Training Step: 64...  Training loss: 3.1619...  0.5196 sec/batch\n",
      "Epoch: 2/40...  Training Step: 65...  Training loss: 3.1408...  0.5152 sec/batch\n",
      "Epoch: 2/40...  Training Step: 66...  Training loss: 3.1485...  0.5235 sec/batch\n",
      "Epoch: 2/40...  Training Step: 67...  Training loss: 3.1773...  0.5331 sec/batch\n",
      "Epoch: 2/40...  Training Step: 68...  Training loss: 3.1440...  0.5243 sec/batch\n",
      "Epoch: 2/40...  Training Step: 69...  Training loss: 3.1540...  0.5276 sec/batch\n",
      "Epoch: 2/40...  Training Step: 70...  Training loss: 3.1228...  0.5155 sec/batch\n",
      "Epoch: 2/40...  Training Step: 71...  Training loss: 3.1211...  0.5258 sec/batch\n",
      "Epoch: 2/40...  Training Step: 72...  Training loss: 3.1415...  0.5247 sec/batch\n",
      "Epoch: 2/40...  Training Step: 73...  Training loss: 3.1429...  0.5179 sec/batch\n",
      "Epoch: 2/40...  Training Step: 74...  Training loss: 3.1513...  0.5129 sec/batch\n",
      "Epoch: 2/40...  Training Step: 75...  Training loss: 3.1266...  0.5201 sec/batch\n",
      "Epoch: 2/40...  Training Step: 76...  Training loss: 3.1473...  0.5099 sec/batch\n",
      "Epoch: 2/40...  Training Step: 77...  Training loss: 3.1340...  0.5104 sec/batch\n",
      "Epoch: 2/40...  Training Step: 78...  Training loss: 3.1328...  0.5229 sec/batch\n",
      "Epoch: 2/40...  Training Step: 79...  Training loss: 3.1280...  0.5103 sec/batch\n",
      "Epoch: 2/40...  Training Step: 80...  Training loss: 3.1368...  0.5014 sec/batch\n",
      "Epoch: 3/40...  Training Step: 81...  Training loss: 3.1649...  0.5178 sec/batch\n",
      "Epoch: 3/40...  Training Step: 82...  Training loss: 3.1095...  0.5146 sec/batch\n",
      "Epoch: 3/40...  Training Step: 83...  Training loss: 3.1153...  0.5080 sec/batch\n",
      "Epoch: 3/40...  Training Step: 84...  Training loss: 3.1100...  0.5078 sec/batch\n",
      "Epoch: 3/40...  Training Step: 85...  Training loss: 3.1249...  0.5081 sec/batch\n",
      "Epoch: 3/40...  Training Step: 86...  Training loss: 3.1097...  0.5127 sec/batch\n",
      "Epoch: 3/40...  Training Step: 87...  Training loss: 3.1148...  0.5197 sec/batch\n",
      "Epoch: 3/40...  Training Step: 88...  Training loss: 3.1075...  0.5089 sec/batch\n",
      "Epoch: 3/40...  Training Step: 89...  Training loss: 3.0979...  0.5094 sec/batch\n",
      "Epoch: 3/40...  Training Step: 90...  Training loss: 3.1104...  0.5160 sec/batch\n",
      "Epoch: 3/40...  Training Step: 91...  Training loss: 3.1135...  0.5235 sec/batch\n",
      "Epoch: 3/40...  Training Step: 92...  Training loss: 3.1180...  0.5055 sec/batch\n",
      "Epoch: 3/40...  Training Step: 93...  Training loss: 3.0968...  0.5043 sec/batch\n",
      "Epoch: 3/40...  Training Step: 94...  Training loss: 3.0980...  0.5082 sec/batch\n",
      "Epoch: 3/40...  Training Step: 95...  Training loss: 3.1149...  0.5160 sec/batch\n",
      "Epoch: 3/40...  Training Step: 96...  Training loss: 3.0912...  0.5087 sec/batch\n",
      "Epoch: 3/40...  Training Step: 97...  Training loss: 3.0792...  0.5111 sec/batch\n",
      "Epoch: 3/40...  Training Step: 98...  Training loss: 3.0821...  0.5199 sec/batch\n",
      "Epoch: 3/40...  Training Step: 99...  Training loss: 3.0888...  0.5050 sec/batch\n",
      "Epoch: 3/40...  Training Step: 100...  Training loss: 3.1045...  0.5222 sec/batch\n",
      "Epoch: 3/40...  Training Step: 101...  Training loss: 3.0897...  0.5163 sec/batch\n",
      "Epoch: 3/40...  Training Step: 102...  Training loss: 3.0883...  0.5152 sec/batch\n",
      "Epoch: 3/40...  Training Step: 103...  Training loss: 3.0760...  0.5221 sec/batch\n",
      "Epoch: 3/40...  Training Step: 104...  Training loss: 3.0700...  0.5183 sec/batch\n",
      "Epoch: 3/40...  Training Step: 105...  Training loss: 3.0498...  0.5140 sec/batch\n",
      "Epoch: 3/40...  Training Step: 106...  Training loss: 3.0530...  0.5207 sec/batch\n",
      "Epoch: 3/40...  Training Step: 107...  Training loss: 3.0690...  0.5176 sec/batch\n",
      "Epoch: 3/40...  Training Step: 108...  Training loss: 3.0396...  0.5251 sec/batch\n",
      "Epoch: 3/40...  Training Step: 109...  Training loss: 3.0438...  0.5207 sec/batch\n",
      "Epoch: 3/40...  Training Step: 110...  Training loss: 3.0064...  0.5229 sec/batch\n",
      "Epoch: 3/40...  Training Step: 111...  Training loss: 2.9884...  0.5283 sec/batch\n",
      "Epoch: 3/40...  Training Step: 112...  Training loss: 3.0114...  0.5190 sec/batch\n",
      "Epoch: 3/40...  Training Step: 113...  Training loss: 3.0268...  0.5215 sec/batch\n",
      "Epoch: 3/40...  Training Step: 114...  Training loss: 3.0135...  0.5230 sec/batch\n",
      "Epoch: 3/40...  Training Step: 115...  Training loss: 2.9841...  0.5096 sec/batch\n",
      "Epoch: 3/40...  Training Step: 116...  Training loss: 3.0227...  0.5239 sec/batch\n",
      "Epoch: 3/40...  Training Step: 117...  Training loss: 3.0725...  0.5173 sec/batch\n",
      "Epoch: 3/40...  Training Step: 118...  Training loss: 3.0482...  0.5106 sec/batch\n",
      "Epoch: 3/40...  Training Step: 119...  Training loss: 3.0133...  0.5312 sec/batch\n",
      "Epoch: 3/40...  Training Step: 120...  Training loss: 3.0218...  0.5155 sec/batch\n",
      "Epoch: 4/40...  Training Step: 121...  Training loss: 3.0455...  0.5087 sec/batch\n",
      "Epoch: 4/40...  Training Step: 122...  Training loss: 2.9936...  0.5068 sec/batch\n",
      "Epoch: 4/40...  Training Step: 123...  Training loss: 2.9884...  0.5147 sec/batch\n",
      "Epoch: 4/40...  Training Step: 124...  Training loss: 2.9689...  0.5232 sec/batch\n",
      "Epoch: 4/40...  Training Step: 125...  Training loss: 2.9850...  0.5128 sec/batch\n",
      "Epoch: 4/40...  Training Step: 126...  Training loss: 2.9614...  0.5219 sec/batch\n",
      "Epoch: 4/40...  Training Step: 127...  Training loss: 2.9471...  0.5044 sec/batch\n",
      "Epoch: 4/40...  Training Step: 128...  Training loss: 2.9361...  0.5115 sec/batch\n",
      "Epoch: 4/40...  Training Step: 129...  Training loss: 2.9234...  0.5177 sec/batch\n",
      "Epoch: 4/40...  Training Step: 130...  Training loss: 2.9198...  0.5199 sec/batch\n",
      "Epoch: 4/40...  Training Step: 131...  Training loss: 2.9148...  0.5114 sec/batch\n",
      "Epoch: 4/40...  Training Step: 132...  Training loss: 2.9129...  0.5112 sec/batch\n",
      "Epoch: 4/40...  Training Step: 133...  Training loss: 2.8906...  0.5205 sec/batch\n",
      "Epoch: 4/40...  Training Step: 134...  Training loss: 2.8791...  0.5174 sec/batch\n",
      "Epoch: 4/40...  Training Step: 135...  Training loss: 2.8878...  0.5230 sec/batch\n",
      "Epoch: 4/40...  Training Step: 136...  Training loss: 2.8677...  0.5129 sec/batch\n",
      "Epoch: 4/40...  Training Step: 137...  Training loss: 2.8367...  0.5168 sec/batch\n",
      "Epoch: 4/40...  Training Step: 138...  Training loss: 2.8380...  0.5126 sec/batch\n",
      "Epoch: 4/40...  Training Step: 139...  Training loss: 2.8403...  0.5189 sec/batch\n",
      "Epoch: 4/40...  Training Step: 140...  Training loss: 2.8399...  0.5186 sec/batch\n",
      "Epoch: 4/40...  Training Step: 141...  Training loss: 2.8182...  0.5195 sec/batch\n",
      "Epoch: 4/40...  Training Step: 142...  Training loss: 2.8199...  0.5164 sec/batch\n",
      "Epoch: 4/40...  Training Step: 143...  Training loss: 2.7880...  0.5104 sec/batch\n",
      "Epoch: 4/40...  Training Step: 144...  Training loss: 2.7738...  0.5230 sec/batch\n",
      "Epoch: 4/40...  Training Step: 145...  Training loss: 2.7653...  0.5274 sec/batch\n",
      "Epoch: 4/40...  Training Step: 146...  Training loss: 2.8968...  0.5188 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/40...  Training Step: 147...  Training loss: 2.8906...  0.5196 sec/batch\n",
      "Epoch: 4/40...  Training Step: 148...  Training loss: 2.8127...  0.5219 sec/batch\n",
      "Epoch: 4/40...  Training Step: 149...  Training loss: 2.8384...  0.5201 sec/batch\n",
      "Epoch: 4/40...  Training Step: 150...  Training loss: 2.8046...  0.5020 sec/batch\n",
      "Epoch: 4/40...  Training Step: 151...  Training loss: 2.7773...  0.5133 sec/batch\n",
      "Epoch: 4/40...  Training Step: 152...  Training loss: 2.7845...  0.5125 sec/batch\n",
      "Epoch: 4/40...  Training Step: 153...  Training loss: 2.8048...  0.5186 sec/batch\n",
      "Epoch: 4/40...  Training Step: 154...  Training loss: 2.8010...  0.5213 sec/batch\n",
      "Epoch: 4/40...  Training Step: 155...  Training loss: 2.7561...  0.5172 sec/batch\n",
      "Epoch: 4/40...  Training Step: 156...  Training loss: 2.7707...  0.5072 sec/batch\n",
      "Epoch: 4/40...  Training Step: 157...  Training loss: 2.7768...  0.5178 sec/batch\n",
      "Epoch: 4/40...  Training Step: 158...  Training loss: 2.7691...  0.5189 sec/batch\n",
      "Epoch: 4/40...  Training Step: 159...  Training loss: 2.7450...  0.5197 sec/batch\n",
      "Epoch: 4/40...  Training Step: 160...  Training loss: 2.7241...  0.5186 sec/batch\n",
      "Epoch: 5/40...  Training Step: 161...  Training loss: 2.7578...  0.5246 sec/batch\n",
      "Epoch: 5/40...  Training Step: 162...  Training loss: 2.7084...  0.5114 sec/batch\n",
      "Epoch: 5/40...  Training Step: 163...  Training loss: 2.7038...  0.5242 sec/batch\n",
      "Epoch: 5/40...  Training Step: 164...  Training loss: 2.6943...  0.5061 sec/batch\n",
      "Epoch: 5/40...  Training Step: 165...  Training loss: 2.7087...  0.5207 sec/batch\n",
      "Epoch: 5/40...  Training Step: 166...  Training loss: 2.6910...  0.5150 sec/batch\n",
      "Epoch: 5/40...  Training Step: 167...  Training loss: 2.6775...  0.5217 sec/batch\n",
      "Epoch: 5/40...  Training Step: 168...  Training loss: 2.6654...  0.5079 sec/batch\n",
      "Epoch: 5/40...  Training Step: 169...  Training loss: 2.6669...  0.5175 sec/batch\n",
      "Epoch: 5/40...  Training Step: 170...  Training loss: 2.6687...  0.5125 sec/batch\n",
      "Epoch: 5/40...  Training Step: 171...  Training loss: 2.6705...  0.5146 sec/batch\n",
      "Epoch: 5/40...  Training Step: 172...  Training loss: 2.6585...  0.5089 sec/batch\n",
      "Epoch: 5/40...  Training Step: 173...  Training loss: 2.6487...  0.5216 sec/batch\n",
      "Epoch: 5/40...  Training Step: 174...  Training loss: 2.6560...  0.5069 sec/batch\n",
      "Epoch: 5/40...  Training Step: 175...  Training loss: 2.6641...  0.5080 sec/batch\n",
      "Epoch: 5/40...  Training Step: 176...  Training loss: 2.6488...  0.5169 sec/batch\n",
      "Epoch: 5/40...  Training Step: 177...  Training loss: 2.6205...  0.5126 sec/batch\n",
      "Epoch: 5/40...  Training Step: 178...  Training loss: 2.6227...  0.5172 sec/batch\n",
      "Epoch: 5/40...  Training Step: 179...  Training loss: 2.6358...  0.5179 sec/batch\n",
      "Epoch: 5/40...  Training Step: 180...  Training loss: 2.6409...  0.5105 sec/batch\n",
      "Epoch: 5/40...  Training Step: 181...  Training loss: 2.6373...  0.5226 sec/batch\n",
      "Epoch: 5/40...  Training Step: 182...  Training loss: 2.6404...  0.5194 sec/batch\n",
      "Epoch: 5/40...  Training Step: 183...  Training loss: 2.6067...  0.5252 sec/batch\n",
      "Epoch: 5/40...  Training Step: 184...  Training loss: 2.5878...  0.5293 sec/batch\n",
      "Epoch: 5/40...  Training Step: 185...  Training loss: 2.5728...  0.5160 sec/batch\n",
      "Epoch: 5/40...  Training Step: 186...  Training loss: 2.5872...  0.5197 sec/batch\n",
      "Epoch: 5/40...  Training Step: 187...  Training loss: 2.5985...  0.5306 sec/batch\n",
      "Epoch: 5/40...  Training Step: 188...  Training loss: 2.5803...  0.5134 sec/batch\n",
      "Epoch: 5/40...  Training Step: 189...  Training loss: 2.5904...  0.5234 sec/batch\n",
      "Epoch: 5/40...  Training Step: 190...  Training loss: 2.5627...  0.5227 sec/batch\n",
      "Epoch: 5/40...  Training Step: 191...  Training loss: 2.5467...  0.5178 sec/batch\n",
      "Epoch: 5/40...  Training Step: 192...  Training loss: 2.5766...  0.5282 sec/batch\n",
      "Epoch: 5/40...  Training Step: 193...  Training loss: 2.5961...  0.5215 sec/batch\n",
      "Epoch: 5/40...  Training Step: 194...  Training loss: 2.5879...  0.5186 sec/batch\n",
      "Epoch: 5/40...  Training Step: 195...  Training loss: 2.5706...  0.5148 sec/batch\n",
      "Epoch: 5/40...  Training Step: 196...  Training loss: 2.5887...  0.5209 sec/batch\n",
      "Epoch: 5/40...  Training Step: 197...  Training loss: 2.5942...  0.4995 sec/batch\n",
      "Epoch: 5/40...  Training Step: 198...  Training loss: 2.5995...  0.5173 sec/batch\n",
      "Epoch: 5/40...  Training Step: 199...  Training loss: 2.5812...  0.5194 sec/batch\n",
      "Epoch: 5/40...  Training Step: 200...  Training loss: 2.5679...  0.5049 sec/batch\n",
      "Epoch: 6/40...  Training Step: 201...  Training loss: 2.5976...  0.5199 sec/batch\n",
      "Epoch: 6/40...  Training Step: 202...  Training loss: 2.5509...  0.5157 sec/batch\n",
      "Epoch: 6/40...  Training Step: 203...  Training loss: 2.5481...  0.5136 sec/batch\n",
      "Epoch: 6/40...  Training Step: 204...  Training loss: 2.5377...  0.5181 sec/batch\n",
      "Epoch: 6/40...  Training Step: 205...  Training loss: 2.5566...  0.5162 sec/batch\n",
      "Epoch: 6/40...  Training Step: 206...  Training loss: 2.5475...  0.5185 sec/batch\n",
      "Epoch: 6/40...  Training Step: 207...  Training loss: 2.5292...  0.5145 sec/batch\n",
      "Epoch: 6/40...  Training Step: 208...  Training loss: 2.5331...  0.5136 sec/batch\n",
      "Epoch: 6/40...  Training Step: 209...  Training loss: 2.5276...  0.5056 sec/batch\n",
      "Epoch: 6/40...  Training Step: 210...  Training loss: 2.5308...  0.5139 sec/batch\n",
      "Epoch: 6/40...  Training Step: 211...  Training loss: 2.5366...  0.5232 sec/batch\n",
      "Epoch: 6/40...  Training Step: 212...  Training loss: 2.5302...  0.5214 sec/batch\n",
      "Epoch: 6/40...  Training Step: 213...  Training loss: 2.5269...  0.5250 sec/batch\n",
      "Epoch: 6/40...  Training Step: 214...  Training loss: 2.5350...  0.5096 sec/batch\n",
      "Epoch: 6/40...  Training Step: 215...  Training loss: 2.5395...  0.5119 sec/batch\n",
      "Epoch: 6/40...  Training Step: 216...  Training loss: 2.5255...  0.5144 sec/batch\n",
      "Epoch: 6/40...  Training Step: 217...  Training loss: 2.5012...  0.5108 sec/batch\n",
      "Epoch: 6/40...  Training Step: 218...  Training loss: 2.4928...  0.5142 sec/batch\n",
      "Epoch: 6/40...  Training Step: 219...  Training loss: 2.5138...  0.5110 sec/batch\n",
      "Epoch: 6/40...  Training Step: 220...  Training loss: 2.5182...  0.5242 sec/batch\n",
      "Epoch: 6/40...  Training Step: 221...  Training loss: 2.5243...  0.5096 sec/batch\n",
      "Epoch: 6/40...  Training Step: 222...  Training loss: 2.5239...  0.5144 sec/batch\n",
      "Epoch: 6/40...  Training Step: 223...  Training loss: 2.4994...  0.5157 sec/batch\n",
      "Epoch: 6/40...  Training Step: 224...  Training loss: 2.4773...  0.5126 sec/batch\n",
      "Epoch: 6/40...  Training Step: 225...  Training loss: 2.4634...  0.5072 sec/batch\n",
      "Epoch: 6/40...  Training Step: 226...  Training loss: 2.4804...  0.5086 sec/batch\n",
      "Epoch: 6/40...  Training Step: 227...  Training loss: 2.4932...  0.5095 sec/batch\n",
      "Epoch: 6/40...  Training Step: 228...  Training loss: 2.4659...  0.5225 sec/batch\n",
      "Epoch: 6/40...  Training Step: 229...  Training loss: 2.4853...  0.5309 sec/batch\n",
      "Epoch: 6/40...  Training Step: 230...  Training loss: 2.4557...  0.5255 sec/batch\n",
      "Epoch: 6/40...  Training Step: 231...  Training loss: 2.4475...  0.5241 sec/batch\n",
      "Epoch: 6/40...  Training Step: 232...  Training loss: 2.4783...  0.5208 sec/batch\n",
      "Epoch: 6/40...  Training Step: 233...  Training loss: 2.4818...  0.5250 sec/batch\n",
      "Epoch: 6/40...  Training Step: 234...  Training loss: 2.4900...  0.5249 sec/batch\n",
      "Epoch: 6/40...  Training Step: 235...  Training loss: 2.4612...  0.5247 sec/batch\n",
      "Epoch: 6/40...  Training Step: 236...  Training loss: 2.4806...  0.5074 sec/batch\n",
      "Epoch: 6/40...  Training Step: 237...  Training loss: 2.4874...  0.5191 sec/batch\n",
      "Epoch: 6/40...  Training Step: 238...  Training loss: 2.4932...  0.5217 sec/batch\n",
      "Epoch: 6/40...  Training Step: 239...  Training loss: 2.4810...  0.5149 sec/batch\n",
      "Epoch: 6/40...  Training Step: 240...  Training loss: 2.4752...  0.5176 sec/batch\n",
      "Epoch: 7/40...  Training Step: 241...  Training loss: 2.5269...  0.5057 sec/batch\n",
      "Epoch: 7/40...  Training Step: 242...  Training loss: 2.4626...  0.5084 sec/batch\n",
      "Epoch: 7/40...  Training Step: 243...  Training loss: 2.4629...  0.5099 sec/batch\n",
      "Epoch: 7/40...  Training Step: 244...  Training loss: 2.4480...  0.5238 sec/batch\n",
      "Epoch: 7/40...  Training Step: 245...  Training loss: 2.4712...  0.5191 sec/batch\n",
      "Epoch: 7/40...  Training Step: 246...  Training loss: 2.4525...  0.5245 sec/batch\n",
      "Epoch: 7/40...  Training Step: 247...  Training loss: 2.4476...  0.5152 sec/batch\n",
      "Epoch: 7/40...  Training Step: 248...  Training loss: 2.4455...  0.5240 sec/batch\n",
      "Epoch: 7/40...  Training Step: 249...  Training loss: 2.4398...  0.5138 sec/batch\n",
      "Epoch: 7/40...  Training Step: 250...  Training loss: 2.4521...  0.5231 sec/batch\n",
      "Epoch: 7/40...  Training Step: 251...  Training loss: 2.4474...  0.5193 sec/batch\n",
      "Epoch: 7/40...  Training Step: 252...  Training loss: 2.4417...  0.5159 sec/batch\n",
      "Epoch: 7/40...  Training Step: 253...  Training loss: 2.4369...  0.5212 sec/batch\n",
      "Epoch: 7/40...  Training Step: 254...  Training loss: 2.4486...  0.5310 sec/batch\n",
      "Epoch: 7/40...  Training Step: 255...  Training loss: 2.4561...  0.5269 sec/batch\n",
      "Epoch: 7/40...  Training Step: 256...  Training loss: 2.4365...  0.5167 sec/batch\n",
      "Epoch: 7/40...  Training Step: 257...  Training loss: 2.4144...  0.5267 sec/batch\n",
      "Epoch: 7/40...  Training Step: 258...  Training loss: 2.4109...  0.5249 sec/batch\n",
      "Epoch: 7/40...  Training Step: 259...  Training loss: 2.4272...  0.5288 sec/batch\n",
      "Epoch: 7/40...  Training Step: 260...  Training loss: 2.4328...  0.5020 sec/batch\n",
      "Epoch: 7/40...  Training Step: 261...  Training loss: 2.4400...  0.5041 sec/batch\n",
      "Epoch: 7/40...  Training Step: 262...  Training loss: 2.4339...  0.5101 sec/batch\n",
      "Epoch: 7/40...  Training Step: 263...  Training loss: 2.4137...  0.5262 sec/batch\n",
      "Epoch: 7/40...  Training Step: 264...  Training loss: 2.3980...  0.5176 sec/batch\n",
      "Epoch: 7/40...  Training Step: 265...  Training loss: 2.3767...  0.5162 sec/batch\n",
      "Epoch: 7/40...  Training Step: 266...  Training loss: 2.4006...  0.5071 sec/batch\n",
      "Epoch: 7/40...  Training Step: 267...  Training loss: 2.4042...  0.5082 sec/batch\n",
      "Epoch: 7/40...  Training Step: 268...  Training loss: 2.3905...  0.5099 sec/batch\n",
      "Epoch: 7/40...  Training Step: 269...  Training loss: 2.4022...  0.5194 sec/batch\n",
      "Epoch: 7/40...  Training Step: 270...  Training loss: 2.3760...  0.5261 sec/batch\n",
      "Epoch: 7/40...  Training Step: 271...  Training loss: 2.3688...  0.5135 sec/batch\n",
      "Epoch: 7/40...  Training Step: 272...  Training loss: 2.3970...  0.5216 sec/batch\n",
      "Epoch: 7/40...  Training Step: 273...  Training loss: 2.3961...  0.5258 sec/batch\n",
      "Epoch: 7/40...  Training Step: 274...  Training loss: 2.4144...  0.5219 sec/batch\n",
      "Epoch: 7/40...  Training Step: 275...  Training loss: 2.3847...  0.5254 sec/batch\n",
      "Epoch: 7/40...  Training Step: 276...  Training loss: 2.4033...  0.5162 sec/batch\n",
      "Epoch: 7/40...  Training Step: 277...  Training loss: 2.3989...  0.4985 sec/batch\n",
      "Epoch: 7/40...  Training Step: 278...  Training loss: 2.4140...  0.5217 sec/batch\n",
      "Epoch: 7/40...  Training Step: 279...  Training loss: 2.3959...  0.5196 sec/batch\n",
      "Epoch: 7/40...  Training Step: 280...  Training loss: 2.3872...  0.5292 sec/batch\n",
      "Epoch: 8/40...  Training Step: 281...  Training loss: 2.4250...  0.5192 sec/batch\n",
      "Epoch: 8/40...  Training Step: 282...  Training loss: 2.3756...  0.5144 sec/batch\n",
      "Epoch: 8/40...  Training Step: 283...  Training loss: 2.3795...  0.5113 sec/batch\n",
      "Epoch: 8/40...  Training Step: 284...  Training loss: 2.3649...  0.5053 sec/batch\n",
      "Epoch: 8/40...  Training Step: 285...  Training loss: 2.3894...  0.5158 sec/batch\n",
      "Epoch: 8/40...  Training Step: 286...  Training loss: 2.3646...  0.5292 sec/batch\n",
      "Epoch: 8/40...  Training Step: 287...  Training loss: 2.3456...  0.5242 sec/batch\n",
      "Epoch: 8/40...  Training Step: 288...  Training loss: 2.3548...  0.5193 sec/batch\n",
      "Epoch: 8/40...  Training Step: 289...  Training loss: 2.3562...  0.5242 sec/batch\n",
      "Epoch: 8/40...  Training Step: 290...  Training loss: 2.3698...  0.5213 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/40...  Training Step: 291...  Training loss: 2.3638...  0.5136 sec/batch\n",
      "Epoch: 8/40...  Training Step: 292...  Training loss: 2.3578...  0.5151 sec/batch\n",
      "Epoch: 8/40...  Training Step: 293...  Training loss: 2.3565...  0.5168 sec/batch\n",
      "Epoch: 8/40...  Training Step: 294...  Training loss: 2.3663...  0.5160 sec/batch\n",
      "Epoch: 8/40...  Training Step: 295...  Training loss: 2.3750...  0.5288 sec/batch\n",
      "Epoch: 8/40...  Training Step: 296...  Training loss: 2.3547...  0.5243 sec/batch\n",
      "Epoch: 8/40...  Training Step: 297...  Training loss: 2.3377...  0.5318 sec/batch\n",
      "Epoch: 8/40...  Training Step: 298...  Training loss: 2.3315...  0.5071 sec/batch\n",
      "Epoch: 8/40...  Training Step: 299...  Training loss: 2.3521...  0.5226 sec/batch\n",
      "Epoch: 8/40...  Training Step: 300...  Training loss: 2.3488...  0.5193 sec/batch\n",
      "Epoch: 8/40...  Training Step: 301...  Training loss: 2.3607...  0.5172 sec/batch\n",
      "Epoch: 8/40...  Training Step: 302...  Training loss: 2.3607...  0.5224 sec/batch\n",
      "Epoch: 8/40...  Training Step: 303...  Training loss: 2.3417...  0.5236 sec/batch\n",
      "Epoch: 8/40...  Training Step: 304...  Training loss: 2.3172...  0.5120 sec/batch\n",
      "Epoch: 8/40...  Training Step: 305...  Training loss: 2.3044...  0.5215 sec/batch\n",
      "Epoch: 8/40...  Training Step: 306...  Training loss: 2.3201...  0.5173 sec/batch\n",
      "Epoch: 8/40...  Training Step: 307...  Training loss: 2.3305...  0.5208 sec/batch\n",
      "Epoch: 8/40...  Training Step: 308...  Training loss: 2.3098...  0.5197 sec/batch\n",
      "Epoch: 8/40...  Training Step: 309...  Training loss: 2.3259...  0.5260 sec/batch\n",
      "Epoch: 8/40...  Training Step: 310...  Training loss: 2.3026...  0.5164 sec/batch\n",
      "Epoch: 8/40...  Training Step: 311...  Training loss: 2.2949...  0.5067 sec/batch\n",
      "Epoch: 8/40...  Training Step: 312...  Training loss: 2.3175...  0.5026 sec/batch\n",
      "Epoch: 8/40...  Training Step: 313...  Training loss: 2.3286...  0.5059 sec/batch\n",
      "Epoch: 8/40...  Training Step: 314...  Training loss: 2.3413...  0.5226 sec/batch\n",
      "Epoch: 8/40...  Training Step: 315...  Training loss: 2.3034...  0.5118 sec/batch\n",
      "Epoch: 8/40...  Training Step: 316...  Training loss: 2.3235...  0.5182 sec/batch\n",
      "Epoch: 8/40...  Training Step: 317...  Training loss: 2.3190...  0.5153 sec/batch\n",
      "Epoch: 8/40...  Training Step: 318...  Training loss: 2.3453...  0.5200 sec/batch\n",
      "Epoch: 8/40...  Training Step: 319...  Training loss: 2.3206...  0.5066 sec/batch\n",
      "Epoch: 8/40...  Training Step: 320...  Training loss: 2.3171...  0.5170 sec/batch\n",
      "Epoch: 9/40...  Training Step: 321...  Training loss: 2.3543...  0.5181 sec/batch\n",
      "Epoch: 9/40...  Training Step: 322...  Training loss: 2.3008...  0.5027 sec/batch\n",
      "Epoch: 9/40...  Training Step: 323...  Training loss: 2.3077...  0.5171 sec/batch\n",
      "Epoch: 9/40...  Training Step: 324...  Training loss: 2.2903...  0.5208 sec/batch\n",
      "Epoch: 9/40...  Training Step: 325...  Training loss: 2.3202...  0.5075 sec/batch\n",
      "Epoch: 9/40...  Training Step: 326...  Training loss: 2.2932...  0.5168 sec/batch\n",
      "Epoch: 9/40...  Training Step: 327...  Training loss: 2.2847...  0.5094 sec/batch\n",
      "Epoch: 9/40...  Training Step: 328...  Training loss: 2.2898...  0.5250 sec/batch\n",
      "Epoch: 9/40...  Training Step: 329...  Training loss: 2.2822...  0.5261 sec/batch\n",
      "Epoch: 9/40...  Training Step: 330...  Training loss: 2.2955...  0.5183 sec/batch\n",
      "Epoch: 9/40...  Training Step: 331...  Training loss: 2.2886...  0.5165 sec/batch\n",
      "Epoch: 9/40...  Training Step: 332...  Training loss: 2.2893...  0.5077 sec/batch\n",
      "Epoch: 9/40...  Training Step: 333...  Training loss: 2.2845...  0.5116 sec/batch\n",
      "Epoch: 9/40...  Training Step: 334...  Training loss: 2.3004...  0.5208 sec/batch\n",
      "Epoch: 9/40...  Training Step: 335...  Training loss: 2.3021...  0.5124 sec/batch\n",
      "Epoch: 9/40...  Training Step: 336...  Training loss: 2.2802...  0.5217 sec/batch\n",
      "Epoch: 9/40...  Training Step: 337...  Training loss: 2.2598...  0.5223 sec/batch\n",
      "Epoch: 9/40...  Training Step: 338...  Training loss: 2.2606...  0.5202 sec/batch\n",
      "Epoch: 9/40...  Training Step: 339...  Training loss: 2.2859...  0.5227 sec/batch\n",
      "Epoch: 9/40...  Training Step: 340...  Training loss: 2.2691...  0.5065 sec/batch\n",
      "Epoch: 9/40...  Training Step: 341...  Training loss: 2.2854...  0.5102 sec/batch\n",
      "Epoch: 9/40...  Training Step: 342...  Training loss: 2.2912...  0.5230 sec/batch\n",
      "Epoch: 9/40...  Training Step: 343...  Training loss: 2.2632...  0.5081 sec/batch\n",
      "Epoch: 9/40...  Training Step: 344...  Training loss: 2.2546...  0.5175 sec/batch\n",
      "Epoch: 9/40...  Training Step: 345...  Training loss: 2.2358...  0.5239 sec/batch\n",
      "Epoch: 9/40...  Training Step: 346...  Training loss: 2.2587...  0.5176 sec/batch\n",
      "Epoch: 9/40...  Training Step: 347...  Training loss: 2.2615...  0.5198 sec/batch\n",
      "Epoch: 9/40...  Training Step: 348...  Training loss: 2.2448...  0.5052 sec/batch\n",
      "Epoch: 9/40...  Training Step: 349...  Training loss: 2.2595...  0.5116 sec/batch\n",
      "Epoch: 9/40...  Training Step: 350...  Training loss: 2.2364...  0.5207 sec/batch\n",
      "Epoch: 9/40...  Training Step: 351...  Training loss: 2.2236...  0.5161 sec/batch\n",
      "Epoch: 9/40...  Training Step: 352...  Training loss: 2.2609...  0.5219 sec/batch\n",
      "Epoch: 9/40...  Training Step: 353...  Training loss: 2.2587...  0.5127 sec/batch\n",
      "Epoch: 9/40...  Training Step: 354...  Training loss: 2.2609...  0.5046 sec/batch\n",
      "Epoch: 9/40...  Training Step: 355...  Training loss: 2.2424...  0.5153 sec/batch\n",
      "Epoch: 9/40...  Training Step: 356...  Training loss: 2.2563...  0.5112 sec/batch\n",
      "Epoch: 9/40...  Training Step: 357...  Training loss: 2.2433...  0.5092 sec/batch\n",
      "Epoch: 9/40...  Training Step: 358...  Training loss: 2.2729...  0.5184 sec/batch\n",
      "Epoch: 9/40...  Training Step: 359...  Training loss: 2.2562...  0.5036 sec/batch\n",
      "Epoch: 9/40...  Training Step: 360...  Training loss: 2.2487...  0.5200 sec/batch\n",
      "Epoch: 10/40...  Training Step: 361...  Training loss: 2.2771...  0.5094 sec/batch\n",
      "Epoch: 10/40...  Training Step: 362...  Training loss: 2.2425...  0.5164 sec/batch\n",
      "Epoch: 10/40...  Training Step: 363...  Training loss: 2.2346...  0.5078 sec/batch\n",
      "Epoch: 10/40...  Training Step: 364...  Training loss: 2.2211...  0.5166 sec/batch\n",
      "Epoch: 10/40...  Training Step: 365...  Training loss: 2.2585...  0.5160 sec/batch\n",
      "Epoch: 10/40...  Training Step: 366...  Training loss: 2.2243...  0.5140 sec/batch\n",
      "Epoch: 10/40...  Training Step: 367...  Training loss: 2.2132...  0.5211 sec/batch\n",
      "Epoch: 10/40...  Training Step: 368...  Training loss: 2.2184...  0.5214 sec/batch\n",
      "Epoch: 10/40...  Training Step: 369...  Training loss: 2.2128...  0.5039 sec/batch\n",
      "Epoch: 10/40...  Training Step: 370...  Training loss: 2.2247...  0.5165 sec/batch\n",
      "Epoch: 10/40...  Training Step: 371...  Training loss: 2.2225...  0.5018 sec/batch\n",
      "Epoch: 10/40...  Training Step: 372...  Training loss: 2.2273...  0.5123 sec/batch\n",
      "Epoch: 10/40...  Training Step: 373...  Training loss: 2.2161...  0.5202 sec/batch\n",
      "Epoch: 10/40...  Training Step: 374...  Training loss: 2.2373...  0.5151 sec/batch\n",
      "Epoch: 10/40...  Training Step: 375...  Training loss: 2.2393...  0.5095 sec/batch\n",
      "Epoch: 10/40...  Training Step: 376...  Training loss: 2.2312...  0.5123 sec/batch\n",
      "Epoch: 10/40...  Training Step: 377...  Training loss: 2.1904...  0.5236 sec/batch\n",
      "Epoch: 10/40...  Training Step: 378...  Training loss: 2.1953...  0.5066 sec/batch\n",
      "Epoch: 10/40...  Training Step: 379...  Training loss: 2.2106...  0.5194 sec/batch\n",
      "Epoch: 10/40...  Training Step: 380...  Training loss: 2.2097...  0.5074 sec/batch\n",
      "Epoch: 10/40...  Training Step: 381...  Training loss: 2.2232...  0.5167 sec/batch\n",
      "Epoch: 10/40...  Training Step: 382...  Training loss: 2.2198...  0.5174 sec/batch\n",
      "Epoch: 10/40...  Training Step: 383...  Training loss: 2.2089...  0.5152 sec/batch\n",
      "Epoch: 10/40...  Training Step: 384...  Training loss: 2.1884...  0.5138 sec/batch\n",
      "Epoch: 10/40...  Training Step: 385...  Training loss: 2.1677...  0.5204 sec/batch\n",
      "Epoch: 10/40...  Training Step: 386...  Training loss: 2.2031...  0.5262 sec/batch\n",
      "Epoch: 10/40...  Training Step: 387...  Training loss: 2.1860...  0.5155 sec/batch\n",
      "Epoch: 10/40...  Training Step: 388...  Training loss: 2.1912...  0.5259 sec/batch\n",
      "Epoch: 10/40...  Training Step: 389...  Training loss: 2.1956...  0.5046 sec/batch\n",
      "Epoch: 10/40...  Training Step: 390...  Training loss: 2.1825...  0.5181 sec/batch\n",
      "Epoch: 10/40...  Training Step: 391...  Training loss: 2.1720...  0.4982 sec/batch\n",
      "Epoch: 10/40...  Training Step: 392...  Training loss: 2.1976...  0.5167 sec/batch\n",
      "Epoch: 10/40...  Training Step: 393...  Training loss: 2.2033...  0.5079 sec/batch\n",
      "Epoch: 10/40...  Training Step: 394...  Training loss: 2.2125...  0.5158 sec/batch\n",
      "Epoch: 10/40...  Training Step: 395...  Training loss: 2.1806...  0.5239 sec/batch\n",
      "Epoch: 10/40...  Training Step: 396...  Training loss: 2.1956...  0.5271 sec/batch\n",
      "Epoch: 10/40...  Training Step: 397...  Training loss: 2.1838...  0.5253 sec/batch\n",
      "Epoch: 10/40...  Training Step: 398...  Training loss: 2.2150...  0.5146 sec/batch\n",
      "Epoch: 10/40...  Training Step: 399...  Training loss: 2.1985...  0.5075 sec/batch\n",
      "Epoch: 10/40...  Training Step: 400...  Training loss: 2.1906...  0.5169 sec/batch\n",
      "Epoch: 11/40...  Training Step: 401...  Training loss: 2.2209...  0.5192 sec/batch\n",
      "Epoch: 11/40...  Training Step: 402...  Training loss: 2.1856...  0.5260 sec/batch\n",
      "Epoch: 11/40...  Training Step: 403...  Training loss: 2.1843...  0.5226 sec/batch\n",
      "Epoch: 11/40...  Training Step: 404...  Training loss: 2.1607...  0.5131 sec/batch\n",
      "Epoch: 11/40...  Training Step: 405...  Training loss: 2.1978...  0.5061 sec/batch\n",
      "Epoch: 11/40...  Training Step: 406...  Training loss: 2.1786...  0.5090 sec/batch\n",
      "Epoch: 11/40...  Training Step: 407...  Training loss: 2.1513...  0.5085 sec/batch\n",
      "Epoch: 11/40...  Training Step: 408...  Training loss: 2.1677...  0.5077 sec/batch\n",
      "Epoch: 11/40...  Training Step: 409...  Training loss: 2.1570...  0.5200 sec/batch\n",
      "Epoch: 11/40...  Training Step: 410...  Training loss: 2.1709...  0.5204 sec/batch\n",
      "Epoch: 11/40...  Training Step: 411...  Training loss: 2.1716...  0.5212 sec/batch\n",
      "Epoch: 11/40...  Training Step: 412...  Training loss: 2.1749...  0.5170 sec/batch\n",
      "Epoch: 11/40...  Training Step: 413...  Training loss: 2.1700...  0.5111 sec/batch\n",
      "Epoch: 11/40...  Training Step: 414...  Training loss: 2.1772...  0.5082 sec/batch\n",
      "Epoch: 11/40...  Training Step: 415...  Training loss: 2.1888...  0.5119 sec/batch\n",
      "Epoch: 11/40...  Training Step: 416...  Training loss: 2.1675...  0.5242 sec/batch\n",
      "Epoch: 11/40...  Training Step: 417...  Training loss: 2.1399...  0.5132 sec/batch\n",
      "Epoch: 11/40...  Training Step: 418...  Training loss: 2.1502...  0.5201 sec/batch\n",
      "Epoch: 11/40...  Training Step: 419...  Training loss: 2.1623...  0.5108 sec/batch\n",
      "Epoch: 11/40...  Training Step: 420...  Training loss: 2.1596...  0.5294 sec/batch\n",
      "Epoch: 11/40...  Training Step: 421...  Training loss: 2.1728...  0.5158 sec/batch\n",
      "Epoch: 11/40...  Training Step: 422...  Training loss: 2.1755...  0.5218 sec/batch\n",
      "Epoch: 11/40...  Training Step: 423...  Training loss: 2.1582...  0.5068 sec/batch\n",
      "Epoch: 11/40...  Training Step: 424...  Training loss: 2.1383...  0.5140 sec/batch\n",
      "Epoch: 11/40...  Training Step: 425...  Training loss: 2.1145...  0.5170 sec/batch\n",
      "Epoch: 11/40...  Training Step: 426...  Training loss: 2.1459...  0.5174 sec/batch\n",
      "Epoch: 11/40...  Training Step: 427...  Training loss: 2.1401...  0.5186 sec/batch\n",
      "Epoch: 11/40...  Training Step: 428...  Training loss: 2.1316...  0.5241 sec/batch\n",
      "Epoch: 11/40...  Training Step: 429...  Training loss: 2.1424...  0.5213 sec/batch\n",
      "Epoch: 11/40...  Training Step: 430...  Training loss: 2.1287...  0.5173 sec/batch\n",
      "Epoch: 11/40...  Training Step: 431...  Training loss: 2.1256...  0.5185 sec/batch\n",
      "Epoch: 11/40...  Training Step: 432...  Training loss: 2.1443...  0.5142 sec/batch\n",
      "Epoch: 11/40...  Training Step: 433...  Training loss: 2.1476...  0.5281 sec/batch\n",
      "Epoch: 11/40...  Training Step: 434...  Training loss: 2.1713...  0.5085 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/40...  Training Step: 435...  Training loss: 2.1360...  0.5152 sec/batch\n",
      "Epoch: 11/40...  Training Step: 436...  Training loss: 2.1371...  0.5159 sec/batch\n",
      "Epoch: 11/40...  Training Step: 437...  Training loss: 2.1302...  0.5244 sec/batch\n",
      "Epoch: 11/40...  Training Step: 438...  Training loss: 2.1702...  0.5108 sec/batch\n",
      "Epoch: 11/40...  Training Step: 439...  Training loss: 2.1516...  0.5061 sec/batch\n",
      "Epoch: 11/40...  Training Step: 440...  Training loss: 2.1354...  0.5236 sec/batch\n",
      "Epoch: 12/40...  Training Step: 441...  Training loss: 2.1720...  0.5229 sec/batch\n",
      "Epoch: 12/40...  Training Step: 442...  Training loss: 2.1251...  0.5200 sec/batch\n",
      "Epoch: 12/40...  Training Step: 443...  Training loss: 2.1293...  0.5286 sec/batch\n",
      "Epoch: 12/40...  Training Step: 444...  Training loss: 2.1181...  0.5066 sec/batch\n",
      "Epoch: 12/40...  Training Step: 445...  Training loss: 2.1527...  0.5154 sec/batch\n",
      "Epoch: 12/40...  Training Step: 446...  Training loss: 2.1130...  0.5196 sec/batch\n",
      "Epoch: 12/40...  Training Step: 447...  Training loss: 2.0966...  0.5206 sec/batch\n",
      "Epoch: 12/40...  Training Step: 448...  Training loss: 2.1130...  0.5077 sec/batch\n",
      "Epoch: 12/40...  Training Step: 449...  Training loss: 2.1163...  0.5138 sec/batch\n",
      "Epoch: 12/40...  Training Step: 450...  Training loss: 2.1145...  0.5093 sec/batch\n",
      "Epoch: 12/40...  Training Step: 451...  Training loss: 2.1268...  0.5104 sec/batch\n",
      "Epoch: 12/40...  Training Step: 452...  Training loss: 2.1254...  0.5134 sec/batch\n",
      "Epoch: 12/40...  Training Step: 453...  Training loss: 2.1173...  0.5195 sec/batch\n",
      "Epoch: 12/40...  Training Step: 454...  Training loss: 2.1298...  0.5002 sec/batch\n",
      "Epoch: 12/40...  Training Step: 455...  Training loss: 2.1430...  0.5126 sec/batch\n",
      "Epoch: 12/40...  Training Step: 456...  Training loss: 2.1159...  0.5112 sec/batch\n",
      "Epoch: 12/40...  Training Step: 457...  Training loss: 2.0926...  0.5146 sec/batch\n",
      "Epoch: 12/40...  Training Step: 458...  Training loss: 2.0913...  0.5188 sec/batch\n",
      "Epoch: 12/40...  Training Step: 459...  Training loss: 2.1085...  0.5167 sec/batch\n",
      "Epoch: 12/40...  Training Step: 460...  Training loss: 2.1117...  0.5145 sec/batch\n",
      "Epoch: 12/40...  Training Step: 461...  Training loss: 2.1171...  0.5212 sec/batch\n",
      "Epoch: 12/40...  Training Step: 462...  Training loss: 2.1282...  0.5051 sec/batch\n",
      "Epoch: 12/40...  Training Step: 463...  Training loss: 2.1046...  0.5191 sec/batch\n",
      "Epoch: 12/40...  Training Step: 464...  Training loss: 2.0961...  0.5217 sec/batch\n",
      "Epoch: 12/40...  Training Step: 465...  Training loss: 2.0700...  0.5189 sec/batch\n",
      "Epoch: 12/40...  Training Step: 466...  Training loss: 2.0985...  0.5229 sec/batch\n",
      "Epoch: 12/40...  Training Step: 467...  Training loss: 2.0960...  0.4983 sec/batch\n",
      "Epoch: 12/40...  Training Step: 468...  Training loss: 2.0921...  0.5202 sec/batch\n",
      "Epoch: 12/40...  Training Step: 469...  Training loss: 2.0956...  0.5148 sec/batch\n",
      "Epoch: 12/40...  Training Step: 470...  Training loss: 2.0833...  0.5136 sec/batch\n",
      "Epoch: 12/40...  Training Step: 471...  Training loss: 2.0781...  0.5201 sec/batch\n",
      "Epoch: 12/40...  Training Step: 472...  Training loss: 2.1109...  0.5198 sec/batch\n",
      "Epoch: 12/40...  Training Step: 473...  Training loss: 2.0995...  0.5119 sec/batch\n",
      "Epoch: 12/40...  Training Step: 474...  Training loss: 2.1195...  0.5097 sec/batch\n",
      "Epoch: 12/40...  Training Step: 475...  Training loss: 2.0796...  0.5131 sec/batch\n",
      "Epoch: 12/40...  Training Step: 476...  Training loss: 2.1027...  0.5049 sec/batch\n",
      "Epoch: 12/40...  Training Step: 477...  Training loss: 2.0798...  0.5063 sec/batch\n",
      "Epoch: 12/40...  Training Step: 478...  Training loss: 2.1222...  0.5161 sec/batch\n",
      "Epoch: 12/40...  Training Step: 479...  Training loss: 2.1054...  0.5217 sec/batch\n",
      "Epoch: 12/40...  Training Step: 480...  Training loss: 2.0911...  0.5169 sec/batch\n",
      "Epoch: 13/40...  Training Step: 481...  Training loss: 2.1172...  0.5057 sec/batch\n",
      "Epoch: 13/40...  Training Step: 482...  Training loss: 2.0869...  0.5199 sec/batch\n",
      "Epoch: 13/40...  Training Step: 483...  Training loss: 2.0867...  0.5217 sec/batch\n",
      "Epoch: 13/40...  Training Step: 484...  Training loss: 2.0678...  0.5222 sec/batch\n",
      "Epoch: 13/40...  Training Step: 485...  Training loss: 2.1033...  0.5140 sec/batch\n",
      "Epoch: 13/40...  Training Step: 486...  Training loss: 2.0732...  0.5194 sec/batch\n",
      "Epoch: 13/40...  Training Step: 487...  Training loss: 2.0532...  0.5226 sec/batch\n",
      "Epoch: 13/40...  Training Step: 488...  Training loss: 2.0641...  0.5093 sec/batch\n",
      "Epoch: 13/40...  Training Step: 489...  Training loss: 2.0560...  0.5198 sec/batch\n",
      "Epoch: 13/40...  Training Step: 490...  Training loss: 2.0725...  0.5169 sec/batch\n",
      "Epoch: 13/40...  Training Step: 491...  Training loss: 2.0811...  0.5094 sec/batch\n",
      "Epoch: 13/40...  Training Step: 492...  Training loss: 2.0861...  0.5190 sec/batch\n",
      "Epoch: 13/40...  Training Step: 493...  Training loss: 2.0719...  0.5227 sec/batch\n",
      "Epoch: 13/40...  Training Step: 494...  Training loss: 2.0809...  0.5250 sec/batch\n",
      "Epoch: 13/40...  Training Step: 495...  Training loss: 2.0918...  0.5202 sec/batch\n",
      "Epoch: 13/40...  Training Step: 496...  Training loss: 2.0777...  0.5127 sec/batch\n",
      "Epoch: 13/40...  Training Step: 497...  Training loss: 2.0434...  0.5256 sec/batch\n",
      "Epoch: 13/40...  Training Step: 498...  Training loss: 2.0451...  0.5180 sec/batch\n",
      "Epoch: 13/40...  Training Step: 499...  Training loss: 2.0600...  0.5093 sec/batch\n",
      "Epoch: 13/40...  Training Step: 500...  Training loss: 2.0645...  0.5215 sec/batch\n",
      "Epoch: 13/40...  Training Step: 501...  Training loss: 2.0684...  0.5155 sec/batch\n",
      "Epoch: 13/40...  Training Step: 502...  Training loss: 2.0772...  0.5090 sec/batch\n",
      "Epoch: 13/40...  Training Step: 503...  Training loss: 2.0663...  0.5088 sec/batch\n",
      "Epoch: 13/40...  Training Step: 504...  Training loss: 2.0424...  0.5084 sec/batch\n",
      "Epoch: 13/40...  Training Step: 505...  Training loss: 2.0169...  0.5255 sec/batch\n",
      "Epoch: 13/40...  Training Step: 506...  Training loss: 2.0561...  0.5096 sec/batch\n",
      "Epoch: 13/40...  Training Step: 507...  Training loss: 2.0522...  0.5146 sec/batch\n",
      "Epoch: 13/40...  Training Step: 508...  Training loss: 2.0499...  0.5200 sec/batch\n",
      "Epoch: 13/40...  Training Step: 509...  Training loss: 2.0518...  0.5082 sec/batch\n",
      "Epoch: 13/40...  Training Step: 510...  Training loss: 2.0453...  0.5216 sec/batch\n",
      "Epoch: 13/40...  Training Step: 511...  Training loss: 2.0390...  0.5036 sec/batch\n",
      "Epoch: 13/40...  Training Step: 512...  Training loss: 2.0601...  0.5203 sec/batch\n",
      "Epoch: 13/40...  Training Step: 513...  Training loss: 2.0632...  0.5075 sec/batch\n",
      "Epoch: 13/40...  Training Step: 514...  Training loss: 2.0765...  0.5204 sec/batch\n",
      "Epoch: 13/40...  Training Step: 515...  Training loss: 2.0366...  0.5110 sec/batch\n",
      "Epoch: 13/40...  Training Step: 516...  Training loss: 2.0507...  0.5210 sec/batch\n",
      "Epoch: 13/40...  Training Step: 517...  Training loss: 2.0349...  0.5242 sec/batch\n",
      "Epoch: 13/40...  Training Step: 518...  Training loss: 2.0772...  0.5116 sec/batch\n",
      "Epoch: 13/40...  Training Step: 519...  Training loss: 2.0678...  0.5273 sec/batch\n",
      "Epoch: 13/40...  Training Step: 520...  Training loss: 2.0525...  0.5104 sec/batch\n",
      "Epoch: 14/40...  Training Step: 521...  Training loss: 2.0784...  0.5189 sec/batch\n",
      "Epoch: 14/40...  Training Step: 522...  Training loss: 2.0421...  0.5104 sec/batch\n",
      "Epoch: 14/40...  Training Step: 523...  Training loss: 2.0432...  0.5053 sec/batch\n",
      "Epoch: 14/40...  Training Step: 524...  Training loss: 2.0288...  0.5219 sec/batch\n",
      "Epoch: 14/40...  Training Step: 525...  Training loss: 2.0656...  0.5139 sec/batch\n",
      "Epoch: 14/40...  Training Step: 526...  Training loss: 2.0357...  0.5207 sec/batch\n",
      "Epoch: 14/40...  Training Step: 527...  Training loss: 2.0073...  0.5234 sec/batch\n",
      "Epoch: 14/40...  Training Step: 528...  Training loss: 2.0236...  0.5024 sec/batch\n",
      "Epoch: 14/40...  Training Step: 529...  Training loss: 2.0136...  0.5245 sec/batch\n",
      "Epoch: 14/40...  Training Step: 530...  Training loss: 2.0296...  0.5221 sec/batch\n",
      "Epoch: 14/40...  Training Step: 531...  Training loss: 2.0392...  0.5120 sec/batch\n",
      "Epoch: 14/40...  Training Step: 532...  Training loss: 2.0384...  0.5083 sec/batch\n",
      "Epoch: 14/40...  Training Step: 533...  Training loss: 2.0294...  0.5236 sec/batch\n",
      "Epoch: 14/40...  Training Step: 534...  Training loss: 2.0326...  0.5123 sec/batch\n",
      "Epoch: 14/40...  Training Step: 535...  Training loss: 2.0535...  0.5024 sec/batch\n",
      "Epoch: 14/40...  Training Step: 536...  Training loss: 2.0295...  0.5191 sec/batch\n",
      "Epoch: 14/40...  Training Step: 537...  Training loss: 2.0026...  0.5137 sec/batch\n",
      "Epoch: 14/40...  Training Step: 538...  Training loss: 2.0060...  0.5158 sec/batch\n",
      "Epoch: 14/40...  Training Step: 539...  Training loss: 2.0155...  0.5171 sec/batch\n",
      "Epoch: 14/40...  Training Step: 540...  Training loss: 2.0276...  0.5250 sec/batch\n",
      "Epoch: 14/40...  Training Step: 541...  Training loss: 2.0314...  0.5141 sec/batch\n",
      "Epoch: 14/40...  Training Step: 542...  Training loss: 2.0374...  0.5197 sec/batch\n",
      "Epoch: 14/40...  Training Step: 543...  Training loss: 2.0255...  0.5106 sec/batch\n",
      "Epoch: 14/40...  Training Step: 544...  Training loss: 2.0065...  0.5318 sec/batch\n",
      "Epoch: 14/40...  Training Step: 545...  Training loss: 1.9844...  0.5210 sec/batch\n",
      "Epoch: 14/40...  Training Step: 546...  Training loss: 2.0175...  0.5137 sec/batch\n",
      "Epoch: 14/40...  Training Step: 547...  Training loss: 2.0104...  0.5161 sec/batch\n",
      "Epoch: 14/40...  Training Step: 548...  Training loss: 2.0061...  0.5046 sec/batch\n",
      "Epoch: 14/40...  Training Step: 549...  Training loss: 2.0141...  0.5192 sec/batch\n",
      "Epoch: 14/40...  Training Step: 550...  Training loss: 2.0100...  0.5098 sec/batch\n",
      "Epoch: 14/40...  Training Step: 551...  Training loss: 2.0037...  0.5085 sec/batch\n",
      "Epoch: 14/40...  Training Step: 552...  Training loss: 2.0232...  0.5217 sec/batch\n",
      "Epoch: 14/40...  Training Step: 553...  Training loss: 2.0166...  0.5197 sec/batch\n",
      "Epoch: 14/40...  Training Step: 554...  Training loss: 2.0331...  0.5325 sec/batch\n",
      "Epoch: 14/40...  Training Step: 555...  Training loss: 1.9936...  0.5158 sec/batch\n",
      "Epoch: 14/40...  Training Step: 556...  Training loss: 2.0181...  0.5215 sec/batch\n",
      "Epoch: 14/40...  Training Step: 557...  Training loss: 1.9890...  0.5030 sec/batch\n",
      "Epoch: 14/40...  Training Step: 558...  Training loss: 2.0325...  0.5051 sec/batch\n",
      "Epoch: 14/40...  Training Step: 559...  Training loss: 2.0207...  0.5206 sec/batch\n",
      "Epoch: 14/40...  Training Step: 560...  Training loss: 2.0012...  0.5192 sec/batch\n",
      "Epoch: 15/40...  Training Step: 561...  Training loss: 2.0384...  0.5247 sec/batch\n",
      "Epoch: 15/40...  Training Step: 562...  Training loss: 1.9963...  0.5145 sec/batch\n",
      "Epoch: 15/40...  Training Step: 563...  Training loss: 2.0002...  0.5219 sec/batch\n",
      "Epoch: 15/40...  Training Step: 564...  Training loss: 1.9849...  0.5220 sec/batch\n",
      "Epoch: 15/40...  Training Step: 565...  Training loss: 2.0208...  0.5169 sec/batch\n",
      "Epoch: 15/40...  Training Step: 566...  Training loss: 1.9841...  0.5186 sec/batch\n",
      "Epoch: 15/40...  Training Step: 567...  Training loss: 1.9609...  0.5182 sec/batch\n",
      "Epoch: 15/40...  Training Step: 568...  Training loss: 1.9723...  0.5140 sec/batch\n",
      "Epoch: 15/40...  Training Step: 569...  Training loss: 1.9777...  0.5210 sec/batch\n",
      "Epoch: 15/40...  Training Step: 570...  Training loss: 1.9817...  0.5212 sec/batch\n",
      "Epoch: 15/40...  Training Step: 571...  Training loss: 1.9996...  0.5145 sec/batch\n",
      "Epoch: 15/40...  Training Step: 572...  Training loss: 1.9935...  0.5295 sec/batch\n",
      "Epoch: 15/40...  Training Step: 573...  Training loss: 1.9803...  0.5126 sec/batch\n",
      "Epoch: 15/40...  Training Step: 574...  Training loss: 1.9957...  0.5233 sec/batch\n",
      "Epoch: 15/40...  Training Step: 575...  Training loss: 2.0082...  0.5250 sec/batch\n",
      "Epoch: 15/40...  Training Step: 576...  Training loss: 1.9908...  0.5056 sec/batch\n",
      "Epoch: 15/40...  Training Step: 577...  Training loss: 1.9599...  0.5180 sec/batch\n",
      "Epoch: 15/40...  Training Step: 578...  Training loss: 1.9581...  0.5055 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/40...  Training Step: 579...  Training loss: 1.9775...  0.5137 sec/batch\n",
      "Epoch: 15/40...  Training Step: 580...  Training loss: 1.9869...  0.5205 sec/batch\n",
      "Epoch: 15/40...  Training Step: 581...  Training loss: 1.9834...  0.5200 sec/batch\n",
      "Epoch: 15/40...  Training Step: 582...  Training loss: 1.9962...  0.5203 sec/batch\n",
      "Epoch: 15/40...  Training Step: 583...  Training loss: 1.9870...  0.5232 sec/batch\n",
      "Epoch: 15/40...  Training Step: 584...  Training loss: 1.9694...  0.5088 sec/batch\n",
      "Epoch: 15/40...  Training Step: 585...  Training loss: 1.9449...  0.5199 sec/batch\n",
      "Epoch: 15/40...  Training Step: 586...  Training loss: 1.9711...  0.5059 sec/batch\n",
      "Epoch: 15/40...  Training Step: 587...  Training loss: 1.9702...  0.5179 sec/batch\n",
      "Epoch: 15/40...  Training Step: 588...  Training loss: 1.9655...  0.5158 sec/batch\n",
      "Epoch: 15/40...  Training Step: 589...  Training loss: 1.9736...  0.5039 sec/batch\n",
      "Epoch: 15/40...  Training Step: 590...  Training loss: 1.9719...  0.5141 sec/batch\n",
      "Epoch: 15/40...  Training Step: 591...  Training loss: 1.9618...  0.5157 sec/batch\n",
      "Epoch: 15/40...  Training Step: 592...  Training loss: 1.9812...  0.5204 sec/batch\n",
      "Epoch: 15/40...  Training Step: 593...  Training loss: 1.9759...  0.5101 sec/batch\n",
      "Epoch: 15/40...  Training Step: 594...  Training loss: 2.0053...  0.5081 sec/batch\n",
      "Epoch: 15/40...  Training Step: 595...  Training loss: 1.9472...  0.5233 sec/batch\n",
      "Epoch: 15/40...  Training Step: 596...  Training loss: 1.9720...  0.5004 sec/batch\n",
      "Epoch: 15/40...  Training Step: 597...  Training loss: 1.9498...  0.5164 sec/batch\n",
      "Epoch: 15/40...  Training Step: 598...  Training loss: 1.9950...  0.5201 sec/batch\n",
      "Epoch: 15/40...  Training Step: 599...  Training loss: 1.9823...  0.5144 sec/batch\n",
      "Epoch: 15/40...  Training Step: 600...  Training loss: 1.9647...  0.5157 sec/batch\n",
      "Epoch: 16/40...  Training Step: 601...  Training loss: 1.9995...  0.5122 sec/batch\n",
      "Epoch: 16/40...  Training Step: 602...  Training loss: 1.9551...  0.5280 sec/batch\n",
      "Epoch: 16/40...  Training Step: 603...  Training loss: 1.9720...  0.5177 sec/batch\n",
      "Epoch: 16/40...  Training Step: 604...  Training loss: 1.9513...  0.5059 sec/batch\n",
      "Epoch: 16/40...  Training Step: 605...  Training loss: 1.9839...  0.5172 sec/batch\n",
      "Epoch: 16/40...  Training Step: 606...  Training loss: 1.9520...  0.5152 sec/batch\n",
      "Epoch: 16/40...  Training Step: 607...  Training loss: 1.9154...  0.5201 sec/batch\n",
      "Epoch: 16/40...  Training Step: 608...  Training loss: 1.9416...  0.5125 sec/batch\n",
      "Epoch: 16/40...  Training Step: 609...  Training loss: 1.9410...  0.5172 sec/batch\n",
      "Epoch: 16/40...  Training Step: 610...  Training loss: 1.9442...  0.5140 sec/batch\n",
      "Epoch: 16/40...  Training Step: 611...  Training loss: 1.9617...  0.5104 sec/batch\n",
      "Epoch: 16/40...  Training Step: 612...  Training loss: 1.9588...  0.5212 sec/batch\n",
      "Epoch: 16/40...  Training Step: 613...  Training loss: 1.9479...  0.5221 sec/batch\n",
      "Epoch: 16/40...  Training Step: 614...  Training loss: 1.9654...  0.5241 sec/batch\n",
      "Epoch: 16/40...  Training Step: 615...  Training loss: 1.9776...  0.5220 sec/batch\n",
      "Epoch: 16/40...  Training Step: 616...  Training loss: 1.9488...  0.5085 sec/batch\n",
      "Epoch: 16/40...  Training Step: 617...  Training loss: 1.9267...  0.5171 sec/batch\n",
      "Epoch: 16/40...  Training Step: 618...  Training loss: 1.9299...  0.5159 sec/batch\n",
      "Epoch: 16/40...  Training Step: 619...  Training loss: 1.9449...  0.5224 sec/batch\n",
      "Epoch: 16/40...  Training Step: 620...  Training loss: 1.9412...  0.5203 sec/batch\n",
      "Epoch: 16/40...  Training Step: 621...  Training loss: 1.9463...  0.5182 sec/batch\n",
      "Epoch: 16/40...  Training Step: 622...  Training loss: 1.9574...  0.5064 sec/batch\n",
      "Epoch: 16/40...  Training Step: 623...  Training loss: 1.9416...  0.5165 sec/batch\n",
      "Epoch: 16/40...  Training Step: 624...  Training loss: 1.9279...  0.5218 sec/batch\n",
      "Epoch: 16/40...  Training Step: 625...  Training loss: 1.8974...  0.5117 sec/batch\n",
      "Epoch: 16/40...  Training Step: 626...  Training loss: 1.9424...  0.5204 sec/batch\n",
      "Epoch: 16/40...  Training Step: 627...  Training loss: 1.9232...  0.5244 sec/batch\n",
      "Epoch: 16/40...  Training Step: 628...  Training loss: 1.9291...  0.5092 sec/batch\n",
      "Epoch: 16/40...  Training Step: 629...  Training loss: 1.9289...  0.5211 sec/batch\n",
      "Epoch: 16/40...  Training Step: 630...  Training loss: 1.9286...  0.5249 sec/batch\n",
      "Epoch: 16/40...  Training Step: 631...  Training loss: 1.9227...  0.5126 sec/batch\n",
      "Epoch: 16/40...  Training Step: 632...  Training loss: 1.9450...  0.5132 sec/batch\n",
      "Epoch: 16/40...  Training Step: 633...  Training loss: 1.9466...  0.5169 sec/batch\n",
      "Epoch: 16/40...  Training Step: 634...  Training loss: 1.9616...  0.5209 sec/batch\n",
      "Epoch: 16/40...  Training Step: 635...  Training loss: 1.9081...  0.5116 sec/batch\n",
      "Epoch: 16/40...  Training Step: 636...  Training loss: 1.9451...  0.5168 sec/batch\n",
      "Epoch: 16/40...  Training Step: 637...  Training loss: 1.9109...  0.5251 sec/batch\n",
      "Epoch: 16/40...  Training Step: 638...  Training loss: 1.9594...  0.5198 sec/batch\n",
      "Epoch: 16/40...  Training Step: 639...  Training loss: 1.9427...  0.5131 sec/batch\n",
      "Epoch: 16/40...  Training Step: 640...  Training loss: 1.9197...  0.5286 sec/batch\n",
      "Epoch: 17/40...  Training Step: 641...  Training loss: 1.9558...  0.5116 sec/batch\n",
      "Epoch: 17/40...  Training Step: 642...  Training loss: 1.9172...  0.5207 sec/batch\n",
      "Epoch: 17/40...  Training Step: 643...  Training loss: 1.9242...  0.5272 sec/batch\n",
      "Epoch: 17/40...  Training Step: 644...  Training loss: 1.9117...  0.5066 sec/batch\n",
      "Epoch: 17/40...  Training Step: 645...  Training loss: 1.9443...  0.5227 sec/batch\n",
      "Epoch: 17/40...  Training Step: 646...  Training loss: 1.9055...  0.5167 sec/batch\n",
      "Epoch: 17/40...  Training Step: 647...  Training loss: 1.8867...  0.5205 sec/batch\n",
      "Epoch: 17/40...  Training Step: 648...  Training loss: 1.9009...  0.5171 sec/batch\n",
      "Epoch: 17/40...  Training Step: 649...  Training loss: 1.9075...  0.5143 sec/batch\n",
      "Epoch: 17/40...  Training Step: 650...  Training loss: 1.9074...  0.5126 sec/batch\n",
      "Epoch: 17/40...  Training Step: 651...  Training loss: 1.9280...  0.5264 sec/batch\n",
      "Epoch: 17/40...  Training Step: 652...  Training loss: 1.9141...  0.5148 sec/batch\n",
      "Epoch: 17/40...  Training Step: 653...  Training loss: 1.9002...  0.5103 sec/batch\n",
      "Epoch: 17/40...  Training Step: 654...  Training loss: 1.9193...  0.5178 sec/batch\n",
      "Epoch: 17/40...  Training Step: 655...  Training loss: 1.9440...  0.5062 sec/batch\n",
      "Epoch: 17/40...  Training Step: 656...  Training loss: 1.9160...  0.5195 sec/batch\n",
      "Epoch: 17/40...  Training Step: 657...  Training loss: 1.8868...  0.5203 sec/batch\n",
      "Epoch: 17/40...  Training Step: 658...  Training loss: 1.8963...  0.5240 sec/batch\n",
      "Epoch: 17/40...  Training Step: 659...  Training loss: 1.9054...  0.5045 sec/batch\n",
      "Epoch: 17/40...  Training Step: 660...  Training loss: 1.9131...  0.5113 sec/batch\n",
      "Epoch: 17/40...  Training Step: 661...  Training loss: 1.9137...  0.5157 sec/batch\n",
      "Epoch: 17/40...  Training Step: 662...  Training loss: 1.9236...  0.5094 sec/batch\n",
      "Epoch: 17/40...  Training Step: 663...  Training loss: 1.9023...  0.5208 sec/batch\n",
      "Epoch: 17/40...  Training Step: 664...  Training loss: 1.8888...  0.5210 sec/batch\n",
      "Epoch: 17/40...  Training Step: 665...  Training loss: 1.8670...  0.5139 sec/batch\n",
      "Epoch: 17/40...  Training Step: 666...  Training loss: 1.9053...  0.5163 sec/batch\n",
      "Epoch: 17/40...  Training Step: 667...  Training loss: 1.8869...  0.5134 sec/batch\n",
      "Epoch: 17/40...  Training Step: 668...  Training loss: 1.8954...  0.5170 sec/batch\n",
      "Epoch: 17/40...  Training Step: 669...  Training loss: 1.8893...  0.5140 sec/batch\n",
      "Epoch: 17/40...  Training Step: 670...  Training loss: 1.8945...  0.5057 sec/batch\n",
      "Epoch: 17/40...  Training Step: 671...  Training loss: 1.8884...  0.5117 sec/batch\n",
      "Epoch: 17/40...  Training Step: 672...  Training loss: 1.9060...  0.5167 sec/batch\n",
      "Epoch: 17/40...  Training Step: 673...  Training loss: 1.9002...  0.5188 sec/batch\n",
      "Epoch: 17/40...  Training Step: 674...  Training loss: 1.9261...  0.5047 sec/batch\n",
      "Epoch: 17/40...  Training Step: 675...  Training loss: 1.8690...  0.5196 sec/batch\n",
      "Epoch: 17/40...  Training Step: 676...  Training loss: 1.8950...  0.5112 sec/batch\n",
      "Epoch: 17/40...  Training Step: 677...  Training loss: 1.8780...  0.5237 sec/batch\n",
      "Epoch: 17/40...  Training Step: 678...  Training loss: 1.9276...  0.5225 sec/batch\n",
      "Epoch: 17/40...  Training Step: 679...  Training loss: 1.9020...  0.5080 sec/batch\n",
      "Epoch: 17/40...  Training Step: 680...  Training loss: 1.8862...  0.5226 sec/batch\n",
      "Epoch: 18/40...  Training Step: 681...  Training loss: 1.9320...  0.5193 sec/batch\n",
      "Epoch: 18/40...  Training Step: 682...  Training loss: 1.8829...  0.5192 sec/batch\n",
      "Epoch: 18/40...  Training Step: 683...  Training loss: 1.9006...  0.5087 sec/batch\n",
      "Epoch: 18/40...  Training Step: 684...  Training loss: 1.8817...  0.5171 sec/batch\n",
      "Epoch: 18/40...  Training Step: 685...  Training loss: 1.9095...  0.5185 sec/batch\n",
      "Epoch: 18/40...  Training Step: 686...  Training loss: 1.8815...  0.5085 sec/batch\n",
      "Epoch: 18/40...  Training Step: 687...  Training loss: 1.8409...  0.5173 sec/batch\n",
      "Epoch: 18/40...  Training Step: 688...  Training loss: 1.8772...  0.5086 sec/batch\n",
      "Epoch: 18/40...  Training Step: 689...  Training loss: 1.8739...  0.5253 sec/batch\n",
      "Epoch: 18/40...  Training Step: 690...  Training loss: 1.8754...  0.5046 sec/batch\n",
      "Epoch: 18/40...  Training Step: 691...  Training loss: 1.8975...  0.5208 sec/batch\n",
      "Epoch: 18/40...  Training Step: 692...  Training loss: 1.8811...  0.5212 sec/batch\n",
      "Epoch: 18/40...  Training Step: 693...  Training loss: 1.8720...  0.5143 sec/batch\n",
      "Epoch: 18/40...  Training Step: 694...  Training loss: 1.8857...  0.5084 sec/batch\n",
      "Epoch: 18/40...  Training Step: 695...  Training loss: 1.9132...  0.5026 sec/batch\n",
      "Epoch: 18/40...  Training Step: 696...  Training loss: 1.8828...  0.5111 sec/batch\n",
      "Epoch: 18/40...  Training Step: 697...  Training loss: 1.8515...  0.5212 sec/batch\n",
      "Epoch: 18/40...  Training Step: 698...  Training loss: 1.8621...  0.5015 sec/batch\n",
      "Epoch: 18/40...  Training Step: 699...  Training loss: 1.8681...  0.5104 sec/batch\n",
      "Epoch: 18/40...  Training Step: 700...  Training loss: 1.8754...  0.5194 sec/batch\n",
      "Epoch: 18/40...  Training Step: 701...  Training loss: 1.8759...  0.5189 sec/batch\n",
      "Epoch: 18/40...  Training Step: 702...  Training loss: 1.8889...  0.5128 sec/batch\n",
      "Epoch: 18/40...  Training Step: 703...  Training loss: 1.8757...  0.5229 sec/batch\n",
      "Epoch: 18/40...  Training Step: 704...  Training loss: 1.8586...  0.5141 sec/batch\n",
      "Epoch: 18/40...  Training Step: 705...  Training loss: 1.8326...  0.5220 sec/batch\n",
      "Epoch: 18/40...  Training Step: 706...  Training loss: 1.8760...  0.5189 sec/batch\n",
      "Epoch: 18/40...  Training Step: 707...  Training loss: 1.8540...  0.5167 sec/batch\n",
      "Epoch: 18/40...  Training Step: 708...  Training loss: 1.8617...  0.5036 sec/batch\n",
      "Epoch: 18/40...  Training Step: 709...  Training loss: 1.8588...  0.5203 sec/batch\n",
      "Epoch: 18/40...  Training Step: 710...  Training loss: 1.8588...  0.5106 sec/batch\n",
      "Epoch: 18/40...  Training Step: 711...  Training loss: 1.8606...  0.5148 sec/batch\n",
      "Epoch: 18/40...  Training Step: 712...  Training loss: 1.8734...  0.5190 sec/batch\n",
      "Epoch: 18/40...  Training Step: 713...  Training loss: 1.8663...  0.5232 sec/batch\n",
      "Epoch: 18/40...  Training Step: 714...  Training loss: 1.8891...  0.5213 sec/batch\n",
      "Epoch: 18/40...  Training Step: 715...  Training loss: 1.8357...  0.5085 sec/batch\n",
      "Epoch: 18/40...  Training Step: 716...  Training loss: 1.8724...  0.5271 sec/batch\n",
      "Epoch: 18/40...  Training Step: 717...  Training loss: 1.8464...  0.5143 sec/batch\n",
      "Epoch: 18/40...  Training Step: 718...  Training loss: 1.8980...  0.5297 sec/batch\n",
      "Epoch: 18/40...  Training Step: 719...  Training loss: 1.8778...  0.5154 sec/batch\n",
      "Epoch: 18/40...  Training Step: 720...  Training loss: 1.8514...  0.5250 sec/batch\n",
      "Epoch: 19/40...  Training Step: 721...  Training loss: 1.8840...  0.5258 sec/batch\n",
      "Epoch: 19/40...  Training Step: 722...  Training loss: 1.8450...  0.5228 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/40...  Training Step: 723...  Training loss: 1.8641...  0.5287 sec/batch\n",
      "Epoch: 19/40...  Training Step: 724...  Training loss: 1.8362...  0.5081 sec/batch\n",
      "Epoch: 19/40...  Training Step: 725...  Training loss: 1.8925...  0.5221 sec/batch\n",
      "Epoch: 19/40...  Training Step: 726...  Training loss: 1.8690...  0.5235 sec/batch\n",
      "Epoch: 19/40...  Training Step: 727...  Training loss: 1.8184...  0.5252 sec/batch\n",
      "Epoch: 19/40...  Training Step: 728...  Training loss: 1.8335...  0.5268 sec/batch\n",
      "Epoch: 19/40...  Training Step: 729...  Training loss: 1.8414...  0.5164 sec/batch\n",
      "Epoch: 19/40...  Training Step: 730...  Training loss: 1.8399...  0.5214 sec/batch\n",
      "Epoch: 19/40...  Training Step: 731...  Training loss: 1.8618...  0.5105 sec/batch\n",
      "Epoch: 19/40...  Training Step: 732...  Training loss: 1.8564...  0.5149 sec/batch\n",
      "Epoch: 19/40...  Training Step: 733...  Training loss: 1.8357...  0.5093 sec/batch\n",
      "Epoch: 19/40...  Training Step: 734...  Training loss: 1.8559...  0.5103 sec/batch\n",
      "Epoch: 19/40...  Training Step: 735...  Training loss: 1.8801...  0.5282 sec/batch\n",
      "Epoch: 19/40...  Training Step: 736...  Training loss: 1.8542...  0.5225 sec/batch\n",
      "Epoch: 19/40...  Training Step: 737...  Training loss: 1.8198...  0.5230 sec/batch\n",
      "Epoch: 19/40...  Training Step: 738...  Training loss: 1.8351...  0.5077 sec/batch\n",
      "Epoch: 19/40...  Training Step: 739...  Training loss: 1.8350...  0.5148 sec/batch\n",
      "Epoch: 19/40...  Training Step: 740...  Training loss: 1.8454...  0.5144 sec/batch\n",
      "Epoch: 19/40...  Training Step: 741...  Training loss: 1.8367...  0.5211 sec/batch\n",
      "Epoch: 19/40...  Training Step: 742...  Training loss: 1.8571...  0.5233 sec/batch\n",
      "Epoch: 19/40...  Training Step: 743...  Training loss: 1.8423...  0.5198 sec/batch\n",
      "Epoch: 19/40...  Training Step: 744...  Training loss: 1.8244...  0.5118 sec/batch\n",
      "Epoch: 19/40...  Training Step: 745...  Training loss: 1.7958...  0.5300 sec/batch\n",
      "Epoch: 19/40...  Training Step: 746...  Training loss: 1.8391...  0.5282 sec/batch\n",
      "Epoch: 19/40...  Training Step: 747...  Training loss: 1.8149...  0.5229 sec/batch\n",
      "Epoch: 19/40...  Training Step: 748...  Training loss: 1.8320...  0.5235 sec/batch\n",
      "Epoch: 19/40...  Training Step: 749...  Training loss: 1.8291...  0.5240 sec/batch\n",
      "Epoch: 19/40...  Training Step: 750...  Training loss: 1.8296...  0.5038 sec/batch\n",
      "Epoch: 19/40...  Training Step: 751...  Training loss: 1.8244...  0.5160 sec/batch\n",
      "Epoch: 19/40...  Training Step: 752...  Training loss: 1.8342...  0.5179 sec/batch\n",
      "Epoch: 19/40...  Training Step: 753...  Training loss: 1.8440...  0.5061 sec/batch\n",
      "Epoch: 19/40...  Training Step: 754...  Training loss: 1.8614...  0.5232 sec/batch\n",
      "Epoch: 19/40...  Training Step: 755...  Training loss: 1.7985...  0.5244 sec/batch\n",
      "Epoch: 19/40...  Training Step: 756...  Training loss: 1.8389...  0.5126 sec/batch\n",
      "Epoch: 19/40...  Training Step: 757...  Training loss: 1.8095...  0.5154 sec/batch\n",
      "Epoch: 19/40...  Training Step: 758...  Training loss: 1.8725...  0.5236 sec/batch\n",
      "Epoch: 19/40...  Training Step: 759...  Training loss: 1.8459...  0.5054 sec/batch\n",
      "Epoch: 19/40...  Training Step: 760...  Training loss: 1.8185...  0.5270 sec/batch\n",
      "Epoch: 20/40...  Training Step: 761...  Training loss: 1.8512...  0.5241 sec/batch\n",
      "Epoch: 20/40...  Training Step: 762...  Training loss: 1.8157...  0.5282 sec/batch\n",
      "Epoch: 20/40...  Training Step: 763...  Training loss: 1.8262...  0.5101 sec/batch\n",
      "Epoch: 20/40...  Training Step: 764...  Training loss: 1.8210...  0.5167 sec/batch\n",
      "Epoch: 20/40...  Training Step: 765...  Training loss: 1.8387...  0.5175 sec/batch\n",
      "Epoch: 20/40...  Training Step: 766...  Training loss: 1.8087...  0.5102 sec/batch\n",
      "Epoch: 20/40...  Training Step: 767...  Training loss: 1.7779...  0.5122 sec/batch\n",
      "Epoch: 20/40...  Training Step: 768...  Training loss: 1.8026...  0.5169 sec/batch\n",
      "Epoch: 20/40...  Training Step: 769...  Training loss: 1.7991...  0.5203 sec/batch\n",
      "Epoch: 20/40...  Training Step: 770...  Training loss: 1.7965...  0.5025 sec/batch\n",
      "Epoch: 20/40...  Training Step: 771...  Training loss: 1.8284...  0.5210 sec/batch\n",
      "Epoch: 20/40...  Training Step: 772...  Training loss: 1.8144...  0.5203 sec/batch\n",
      "Epoch: 20/40...  Training Step: 773...  Training loss: 1.7910...  0.5102 sec/batch\n",
      "Epoch: 20/40...  Training Step: 774...  Training loss: 1.8231...  0.5204 sec/batch\n",
      "Epoch: 20/40...  Training Step: 775...  Training loss: 1.8432...  0.5143 sec/batch\n",
      "Epoch: 20/40...  Training Step: 776...  Training loss: 1.8188...  0.5171 sec/batch\n",
      "Epoch: 20/40...  Training Step: 777...  Training loss: 1.7918...  0.5082 sec/batch\n",
      "Epoch: 20/40...  Training Step: 778...  Training loss: 1.8050...  0.5228 sec/batch\n",
      "Epoch: 20/40...  Training Step: 779...  Training loss: 1.8022...  0.5231 sec/batch\n",
      "Epoch: 20/40...  Training Step: 780...  Training loss: 1.8087...  0.5189 sec/batch\n",
      "Epoch: 20/40...  Training Step: 781...  Training loss: 1.8083...  0.5172 sec/batch\n",
      "Epoch: 20/40...  Training Step: 782...  Training loss: 1.8169...  0.5110 sec/batch\n",
      "Epoch: 20/40...  Training Step: 783...  Training loss: 1.8075...  0.5247 sec/batch\n",
      "Epoch: 20/40...  Training Step: 784...  Training loss: 1.7872...  0.5058 sec/batch\n",
      "Epoch: 20/40...  Training Step: 785...  Training loss: 1.7658...  0.5221 sec/batch\n",
      "Epoch: 20/40...  Training Step: 786...  Training loss: 1.8173...  0.5214 sec/batch\n",
      "Epoch: 20/40...  Training Step: 787...  Training loss: 1.7885...  0.5068 sec/batch\n",
      "Epoch: 20/40...  Training Step: 788...  Training loss: 1.8089...  0.5130 sec/batch\n",
      "Epoch: 20/40...  Training Step: 789...  Training loss: 1.7930...  0.5161 sec/batch\n",
      "Epoch: 20/40...  Training Step: 790...  Training loss: 1.8065...  0.5112 sec/batch\n",
      "Epoch: 20/40...  Training Step: 791...  Training loss: 1.7944...  0.5198 sec/batch\n",
      "Epoch: 20/40...  Training Step: 792...  Training loss: 1.8051...  0.5190 sec/batch\n",
      "Epoch: 20/40...  Training Step: 793...  Training loss: 1.8054...  0.5182 sec/batch\n",
      "Epoch: 20/40...  Training Step: 794...  Training loss: 1.8382...  0.5226 sec/batch\n",
      "Epoch: 20/40...  Training Step: 795...  Training loss: 1.7686...  0.5086 sec/batch\n",
      "Epoch: 20/40...  Training Step: 796...  Training loss: 1.8096...  0.5209 sec/batch\n",
      "Epoch: 20/40...  Training Step: 797...  Training loss: 1.7843...  0.5112 sec/batch\n",
      "Epoch: 20/40...  Training Step: 798...  Training loss: 1.8443...  0.5163 sec/batch\n",
      "Epoch: 20/40...  Training Step: 799...  Training loss: 1.8169...  0.5083 sec/batch\n",
      "Epoch: 20/40...  Training Step: 800...  Training loss: 1.7855...  0.5254 sec/batch\n",
      "Epoch: 21/40...  Training Step: 801...  Training loss: 1.8233...  0.5221 sec/batch\n",
      "Epoch: 21/40...  Training Step: 802...  Training loss: 1.7900...  0.5034 sec/batch\n",
      "Epoch: 21/40...  Training Step: 803...  Training loss: 1.7983...  0.5230 sec/batch\n",
      "Epoch: 21/40...  Training Step: 804...  Training loss: 1.7801...  0.5087 sec/batch\n",
      "Epoch: 21/40...  Training Step: 805...  Training loss: 1.8133...  0.5234 sec/batch\n",
      "Epoch: 21/40...  Training Step: 806...  Training loss: 1.7797...  0.5119 sec/batch\n",
      "Epoch: 21/40...  Training Step: 807...  Training loss: 1.7465...  0.5204 sec/batch\n",
      "Epoch: 21/40...  Training Step: 808...  Training loss: 1.7653...  0.5121 sec/batch\n",
      "Epoch: 21/40...  Training Step: 809...  Training loss: 1.7643...  0.5104 sec/batch\n",
      "Epoch: 21/40...  Training Step: 810...  Training loss: 1.7772...  0.5327 sec/batch\n",
      "Epoch: 21/40...  Training Step: 811...  Training loss: 1.7926...  0.5337 sec/batch\n",
      "Epoch: 21/40...  Training Step: 812...  Training loss: 1.7785...  0.5129 sec/batch\n",
      "Epoch: 21/40...  Training Step: 813...  Training loss: 1.7648...  0.5150 sec/batch\n",
      "Epoch: 21/40...  Training Step: 814...  Training loss: 1.7886...  0.5159 sec/batch\n",
      "Epoch: 21/40...  Training Step: 815...  Training loss: 1.8139...  0.5144 sec/batch\n",
      "Epoch: 21/40...  Training Step: 816...  Training loss: 1.7880...  0.5134 sec/batch\n",
      "Epoch: 21/40...  Training Step: 817...  Training loss: 1.7581...  0.5156 sec/batch\n",
      "Epoch: 21/40...  Training Step: 818...  Training loss: 1.7737...  0.5360 sec/batch\n",
      "Epoch: 21/40...  Training Step: 819...  Training loss: 1.7739...  0.5217 sec/batch\n",
      "Epoch: 21/40...  Training Step: 820...  Training loss: 1.7772...  0.5126 sec/batch\n",
      "Epoch: 21/40...  Training Step: 821...  Training loss: 1.7740...  0.5131 sec/batch\n",
      "Epoch: 21/40...  Training Step: 822...  Training loss: 1.7879...  0.5138 sec/batch\n",
      "Epoch: 21/40...  Training Step: 823...  Training loss: 1.7765...  0.5053 sec/batch\n",
      "Epoch: 21/40...  Training Step: 824...  Training loss: 1.7716...  0.5174 sec/batch\n",
      "Epoch: 21/40...  Training Step: 825...  Training loss: 1.7430...  0.5060 sec/batch\n",
      "Epoch: 21/40...  Training Step: 826...  Training loss: 1.7789...  0.5081 sec/batch\n",
      "Epoch: 21/40...  Training Step: 827...  Training loss: 1.7563...  0.5155 sec/batch\n",
      "Epoch: 21/40...  Training Step: 828...  Training loss: 1.7771...  0.5212 sec/batch\n",
      "Epoch: 21/40...  Training Step: 829...  Training loss: 1.7746...  0.5142 sec/batch\n",
      "Epoch: 21/40...  Training Step: 830...  Training loss: 1.7725...  0.5136 sec/batch\n",
      "Epoch: 21/40...  Training Step: 831...  Training loss: 1.7693...  0.5044 sec/batch\n",
      "Epoch: 21/40...  Training Step: 832...  Training loss: 1.7815...  0.5178 sec/batch\n",
      "Epoch: 21/40...  Training Step: 833...  Training loss: 1.7850...  0.5162 sec/batch\n",
      "Epoch: 21/40...  Training Step: 834...  Training loss: 1.8114...  0.5212 sec/batch\n",
      "Epoch: 21/40...  Training Step: 835...  Training loss: 1.7400...  0.5132 sec/batch\n",
      "Epoch: 21/40...  Training Step: 836...  Training loss: 1.7839...  0.5312 sec/batch\n",
      "Epoch: 21/40...  Training Step: 837...  Training loss: 1.7582...  0.5281 sec/batch\n",
      "Epoch: 21/40...  Training Step: 838...  Training loss: 1.8087...  0.5193 sec/batch\n",
      "Epoch: 21/40...  Training Step: 839...  Training loss: 1.7819...  0.5205 sec/batch\n",
      "Epoch: 21/40...  Training Step: 840...  Training loss: 1.7599...  0.5120 sec/batch\n",
      "Epoch: 22/40...  Training Step: 841...  Training loss: 1.8001...  0.5202 sec/batch\n",
      "Epoch: 22/40...  Training Step: 842...  Training loss: 1.7611...  0.5127 sec/batch\n",
      "Epoch: 22/40...  Training Step: 843...  Training loss: 1.7663...  0.5168 sec/batch\n",
      "Epoch: 22/40...  Training Step: 844...  Training loss: 1.7489...  0.5312 sec/batch\n",
      "Epoch: 22/40...  Training Step: 845...  Training loss: 1.7788...  0.5214 sec/batch\n",
      "Epoch: 22/40...  Training Step: 846...  Training loss: 1.7580...  0.5167 sec/batch\n",
      "Epoch: 22/40...  Training Step: 847...  Training loss: 1.7226...  0.5094 sec/batch\n",
      "Epoch: 22/40...  Training Step: 848...  Training loss: 1.7359...  0.5218 sec/batch\n",
      "Epoch: 22/40...  Training Step: 849...  Training loss: 1.7408...  0.5206 sec/batch\n",
      "Epoch: 22/40...  Training Step: 850...  Training loss: 1.7456...  0.5111 sec/batch\n",
      "Epoch: 22/40...  Training Step: 851...  Training loss: 1.7736...  0.5185 sec/batch\n",
      "Epoch: 22/40...  Training Step: 852...  Training loss: 1.7610...  0.5077 sec/batch\n",
      "Epoch: 22/40...  Training Step: 853...  Training loss: 1.7336...  0.5225 sec/batch\n",
      "Epoch: 22/40...  Training Step: 854...  Training loss: 1.7696...  0.5169 sec/batch\n",
      "Epoch: 22/40...  Training Step: 855...  Training loss: 1.7962...  0.5099 sec/batch\n",
      "Epoch: 22/40...  Training Step: 856...  Training loss: 1.7555...  0.5245 sec/batch\n",
      "Epoch: 22/40...  Training Step: 857...  Training loss: 1.7306...  0.5197 sec/batch\n",
      "Epoch: 22/40...  Training Step: 858...  Training loss: 1.7312...  0.5244 sec/batch\n",
      "Epoch: 22/40...  Training Step: 859...  Training loss: 1.7436...  0.5217 sec/batch\n",
      "Epoch: 22/40...  Training Step: 860...  Training loss: 1.7359...  0.5058 sec/batch\n",
      "Epoch: 22/40...  Training Step: 861...  Training loss: 1.7563...  0.5189 sec/batch\n",
      "Epoch: 22/40...  Training Step: 862...  Training loss: 1.7645...  0.5172 sec/batch\n",
      "Epoch: 22/40...  Training Step: 863...  Training loss: 1.7472...  0.5095 sec/batch\n",
      "Epoch: 22/40...  Training Step: 864...  Training loss: 1.7434...  0.5106 sec/batch\n",
      "Epoch: 22/40...  Training Step: 865...  Training loss: 1.7175...  0.5006 sec/batch\n",
      "Epoch: 22/40...  Training Step: 866...  Training loss: 1.7499...  0.5115 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22/40...  Training Step: 867...  Training loss: 1.7309...  0.5192 sec/batch\n",
      "Epoch: 22/40...  Training Step: 868...  Training loss: 1.7437...  0.5076 sec/batch\n",
      "Epoch: 22/40...  Training Step: 869...  Training loss: 1.7493...  0.5162 sec/batch\n",
      "Epoch: 22/40...  Training Step: 870...  Training loss: 1.7310...  0.5262 sec/batch\n",
      "Epoch: 22/40...  Training Step: 871...  Training loss: 1.7443...  0.5178 sec/batch\n",
      "Epoch: 22/40...  Training Step: 872...  Training loss: 1.7583...  0.5055 sec/batch\n",
      "Epoch: 22/40...  Training Step: 873...  Training loss: 1.7542...  0.5120 sec/batch\n",
      "Epoch: 22/40...  Training Step: 874...  Training loss: 1.7730...  0.5075 sec/batch\n",
      "Epoch: 22/40...  Training Step: 875...  Training loss: 1.7182...  0.5086 sec/batch\n",
      "Epoch: 22/40...  Training Step: 876...  Training loss: 1.7568...  0.5169 sec/batch\n",
      "Epoch: 22/40...  Training Step: 877...  Training loss: 1.7278...  0.5095 sec/batch\n",
      "Epoch: 22/40...  Training Step: 878...  Training loss: 1.7746...  0.5197 sec/batch\n",
      "Epoch: 22/40...  Training Step: 879...  Training loss: 1.7596...  0.5077 sec/batch\n",
      "Epoch: 22/40...  Training Step: 880...  Training loss: 1.7271...  0.5245 sec/batch\n",
      "Epoch: 23/40...  Training Step: 881...  Training loss: 1.7712...  0.5142 sec/batch\n",
      "Epoch: 23/40...  Training Step: 882...  Training loss: 1.7283...  0.5232 sec/batch\n",
      "Epoch: 23/40...  Training Step: 883...  Training loss: 1.7502...  0.5205 sec/batch\n",
      "Epoch: 23/40...  Training Step: 884...  Training loss: 1.7267...  0.5088 sec/batch\n",
      "Epoch: 23/40...  Training Step: 885...  Training loss: 1.7532...  0.5203 sec/batch\n",
      "Epoch: 23/40...  Training Step: 886...  Training loss: 1.7343...  0.5257 sec/batch\n",
      "Epoch: 23/40...  Training Step: 887...  Training loss: 1.6977...  0.5181 sec/batch\n",
      "Epoch: 23/40...  Training Step: 888...  Training loss: 1.7086...  0.5202 sec/batch\n",
      "Epoch: 23/40...  Training Step: 889...  Training loss: 1.7193...  0.5141 sec/batch\n",
      "Epoch: 23/40...  Training Step: 890...  Training loss: 1.7222...  0.5248 sec/batch\n",
      "Epoch: 23/40...  Training Step: 891...  Training loss: 1.7381...  0.5129 sec/batch\n",
      "Epoch: 23/40...  Training Step: 892...  Training loss: 1.7257...  0.5154 sec/batch\n",
      "Epoch: 23/40...  Training Step: 893...  Training loss: 1.7120...  0.5228 sec/batch\n",
      "Epoch: 23/40...  Training Step: 894...  Training loss: 1.7435...  0.5136 sec/batch\n",
      "Epoch: 23/40...  Training Step: 895...  Training loss: 1.7681...  0.5182 sec/batch\n",
      "Epoch: 23/40...  Training Step: 896...  Training loss: 1.7357...  0.5292 sec/batch\n",
      "Epoch: 23/40...  Training Step: 897...  Training loss: 1.7047...  0.5091 sec/batch\n",
      "Epoch: 23/40...  Training Step: 898...  Training loss: 1.7158...  0.5104 sec/batch\n",
      "Epoch: 23/40...  Training Step: 899...  Training loss: 1.7180...  0.5174 sec/batch\n",
      "Epoch: 23/40...  Training Step: 900...  Training loss: 1.7182...  0.5200 sec/batch\n",
      "Epoch: 23/40...  Training Step: 901...  Training loss: 1.7286...  0.5228 sec/batch\n",
      "Epoch: 23/40...  Training Step: 902...  Training loss: 1.7393...  0.5053 sec/batch\n",
      "Epoch: 23/40...  Training Step: 903...  Training loss: 1.7179...  0.5231 sec/batch\n",
      "Epoch: 23/40...  Training Step: 904...  Training loss: 1.7223...  0.5172 sec/batch\n",
      "Epoch: 23/40...  Training Step: 905...  Training loss: 1.6780...  0.5208 sec/batch\n",
      "Epoch: 23/40...  Training Step: 906...  Training loss: 1.7142...  0.5025 sec/batch\n",
      "Epoch: 23/40...  Training Step: 907...  Training loss: 1.7141...  0.5209 sec/batch\n",
      "Epoch: 23/40...  Training Step: 908...  Training loss: 1.7221...  0.5150 sec/batch\n",
      "Epoch: 23/40...  Training Step: 909...  Training loss: 1.7308...  0.5213 sec/batch\n",
      "Epoch: 23/40...  Training Step: 910...  Training loss: 1.7155...  0.5097 sec/batch\n",
      "Epoch: 23/40...  Training Step: 911...  Training loss: 1.7220...  0.5222 sec/batch\n",
      "Epoch: 23/40...  Training Step: 912...  Training loss: 1.7382...  0.5204 sec/batch\n",
      "Epoch: 23/40...  Training Step: 913...  Training loss: 1.7281...  0.5261 sec/batch\n",
      "Epoch: 23/40...  Training Step: 914...  Training loss: 1.7603...  0.5123 sec/batch\n",
      "Epoch: 23/40...  Training Step: 915...  Training loss: 1.6884...  0.5176 sec/batch\n",
      "Epoch: 23/40...  Training Step: 916...  Training loss: 1.7275...  0.5158 sec/batch\n",
      "Epoch: 23/40...  Training Step: 917...  Training loss: 1.7084...  0.5195 sec/batch\n",
      "Epoch: 23/40...  Training Step: 918...  Training loss: 1.7626...  0.5256 sec/batch\n",
      "Epoch: 23/40...  Training Step: 919...  Training loss: 1.7462...  0.5017 sec/batch\n",
      "Epoch: 23/40...  Training Step: 920...  Training loss: 1.7192...  0.5078 sec/batch\n",
      "Epoch: 24/40...  Training Step: 921...  Training loss: 1.7484...  0.5056 sec/batch\n",
      "Epoch: 24/40...  Training Step: 922...  Training loss: 1.7082...  0.5230 sec/batch\n",
      "Epoch: 24/40...  Training Step: 923...  Training loss: 1.7248...  0.5113 sec/batch\n",
      "Epoch: 24/40...  Training Step: 924...  Training loss: 1.7056...  0.5237 sec/batch\n",
      "Epoch: 24/40...  Training Step: 925...  Training loss: 1.7341...  0.5244 sec/batch\n",
      "Epoch: 24/40...  Training Step: 926...  Training loss: 1.7080...  0.5172 sec/batch\n",
      "Epoch: 24/40...  Training Step: 927...  Training loss: 1.6674...  0.5063 sec/batch\n",
      "Epoch: 24/40...  Training Step: 928...  Training loss: 1.6825...  0.5233 sec/batch\n",
      "Epoch: 24/40...  Training Step: 929...  Training loss: 1.6952...  0.5208 sec/batch\n",
      "Epoch: 24/40...  Training Step: 930...  Training loss: 1.7023...  0.5218 sec/batch\n",
      "Epoch: 24/40...  Training Step: 931...  Training loss: 1.7208...  0.5295 sec/batch\n",
      "Epoch: 24/40...  Training Step: 932...  Training loss: 1.7064...  0.5209 sec/batch\n",
      "Epoch: 24/40...  Training Step: 933...  Training loss: 1.6867...  0.5223 sec/batch\n",
      "Epoch: 24/40...  Training Step: 934...  Training loss: 1.7239...  0.5064 sec/batch\n",
      "Epoch: 24/40...  Training Step: 935...  Training loss: 1.7375...  0.5136 sec/batch\n",
      "Epoch: 24/40...  Training Step: 936...  Training loss: 1.7206...  0.5184 sec/batch\n",
      "Epoch: 24/40...  Training Step: 937...  Training loss: 1.6880...  0.5220 sec/batch\n",
      "Epoch: 24/40...  Training Step: 938...  Training loss: 1.6985...  0.5215 sec/batch\n",
      "Epoch: 24/40...  Training Step: 939...  Training loss: 1.6955...  0.5231 sec/batch\n",
      "Epoch: 24/40...  Training Step: 940...  Training loss: 1.6964...  0.5026 sec/batch\n",
      "Epoch: 24/40...  Training Step: 941...  Training loss: 1.7122...  0.5215 sec/batch\n",
      "Epoch: 24/40...  Training Step: 942...  Training loss: 1.7141...  0.5211 sec/batch\n",
      "Epoch: 24/40...  Training Step: 943...  Training loss: 1.6995...  0.5134 sec/batch\n",
      "Epoch: 24/40...  Training Step: 944...  Training loss: 1.6953...  0.5197 sec/batch\n",
      "Epoch: 24/40...  Training Step: 945...  Training loss: 1.6590...  0.5117 sec/batch\n",
      "Epoch: 24/40...  Training Step: 946...  Training loss: 1.7051...  0.5245 sec/batch\n",
      "Epoch: 24/40...  Training Step: 947...  Training loss: 1.6775...  0.5262 sec/batch\n",
      "Epoch: 24/40...  Training Step: 948...  Training loss: 1.6912...  0.5191 sec/batch\n",
      "Epoch: 24/40...  Training Step: 949...  Training loss: 1.6970...  0.5189 sec/batch\n",
      "Epoch: 24/40...  Training Step: 950...  Training loss: 1.6904...  0.5282 sec/batch\n",
      "Epoch: 24/40...  Training Step: 951...  Training loss: 1.6857...  0.5262 sec/batch\n",
      "Epoch: 24/40...  Training Step: 952...  Training loss: 1.6997...  0.5203 sec/batch\n",
      "Epoch: 24/40...  Training Step: 953...  Training loss: 1.6984...  0.5251 sec/batch\n",
      "Epoch: 24/40...  Training Step: 954...  Training loss: 1.7320...  0.5160 sec/batch\n",
      "Epoch: 24/40...  Training Step: 955...  Training loss: 1.6594...  0.5106 sec/batch\n",
      "Epoch: 24/40...  Training Step: 956...  Training loss: 1.7016...  0.5041 sec/batch\n",
      "Epoch: 24/40...  Training Step: 957...  Training loss: 1.6695...  0.5204 sec/batch\n",
      "Epoch: 24/40...  Training Step: 958...  Training loss: 1.7338...  0.5156 sec/batch\n",
      "Epoch: 24/40...  Training Step: 959...  Training loss: 1.7158...  0.5269 sec/batch\n",
      "Epoch: 24/40...  Training Step: 960...  Training loss: 1.6799...  0.5206 sec/batch\n",
      "Epoch: 25/40...  Training Step: 961...  Training loss: 1.7229...  0.5065 sec/batch\n",
      "Epoch: 25/40...  Training Step: 962...  Training loss: 1.6826...  0.5156 sec/batch\n",
      "Epoch: 25/40...  Training Step: 963...  Training loss: 1.7011...  0.5211 sec/batch\n",
      "Epoch: 25/40...  Training Step: 964...  Training loss: 1.6771...  0.5137 sec/batch\n",
      "Epoch: 25/40...  Training Step: 965...  Training loss: 1.7101...  0.5223 sec/batch\n",
      "Epoch: 25/40...  Training Step: 966...  Training loss: 1.6825...  0.5239 sec/batch\n",
      "Epoch: 25/40...  Training Step: 967...  Training loss: 1.6473...  0.5090 sec/batch\n",
      "Epoch: 25/40...  Training Step: 968...  Training loss: 1.6560...  0.5166 sec/batch\n",
      "Epoch: 25/40...  Training Step: 969...  Training loss: 1.6649...  0.5204 sec/batch\n",
      "Epoch: 25/40...  Training Step: 970...  Training loss: 1.6668...  0.5297 sec/batch\n",
      "Epoch: 25/40...  Training Step: 971...  Training loss: 1.6879...  0.5219 sec/batch\n",
      "Epoch: 25/40...  Training Step: 972...  Training loss: 1.6808...  0.5243 sec/batch\n",
      "Epoch: 25/40...  Training Step: 973...  Training loss: 1.6583...  0.5174 sec/batch\n",
      "Epoch: 25/40...  Training Step: 974...  Training loss: 1.6879...  0.5178 sec/batch\n",
      "Epoch: 25/40...  Training Step: 975...  Training loss: 1.7212...  0.5272 sec/batch\n",
      "Epoch: 25/40...  Training Step: 976...  Training loss: 1.7172...  0.5279 sec/batch\n",
      "Epoch: 25/40...  Training Step: 977...  Training loss: 1.6648...  0.5102 sec/batch\n",
      "Epoch: 25/40...  Training Step: 978...  Training loss: 1.6841...  0.5122 sec/batch\n",
      "Epoch: 25/40...  Training Step: 979...  Training loss: 1.6788...  0.5176 sec/batch\n",
      "Epoch: 25/40...  Training Step: 980...  Training loss: 1.6835...  0.5088 sec/batch\n",
      "Epoch: 25/40...  Training Step: 981...  Training loss: 1.6878...  0.5191 sec/batch\n",
      "Epoch: 25/40...  Training Step: 982...  Training loss: 1.7040...  0.5196 sec/batch\n",
      "Epoch: 25/40...  Training Step: 983...  Training loss: 1.6860...  0.5129 sec/batch\n",
      "Epoch: 25/40...  Training Step: 984...  Training loss: 1.6772...  0.5284 sec/batch\n",
      "Epoch: 25/40...  Training Step: 985...  Training loss: 1.6494...  0.5083 sec/batch\n",
      "Epoch: 25/40...  Training Step: 986...  Training loss: 1.6824...  0.5008 sec/batch\n",
      "Epoch: 25/40...  Training Step: 987...  Training loss: 1.6686...  0.5221 sec/batch\n",
      "Epoch: 25/40...  Training Step: 988...  Training loss: 1.6773...  0.5197 sec/batch\n",
      "Epoch: 25/40...  Training Step: 989...  Training loss: 1.6814...  0.5124 sec/batch\n",
      "Epoch: 25/40...  Training Step: 990...  Training loss: 1.6802...  0.5092 sec/batch\n",
      "Epoch: 25/40...  Training Step: 991...  Training loss: 1.6641...  0.5087 sec/batch\n",
      "Epoch: 25/40...  Training Step: 992...  Training loss: 1.6838...  0.5237 sec/batch\n",
      "Epoch: 25/40...  Training Step: 993...  Training loss: 1.6844...  0.5193 sec/batch\n",
      "Epoch: 25/40...  Training Step: 994...  Training loss: 1.7008...  0.5104 sec/batch\n",
      "Epoch: 25/40...  Training Step: 995...  Training loss: 1.6406...  0.5243 sec/batch\n",
      "Epoch: 25/40...  Training Step: 996...  Training loss: 1.6818...  0.5217 sec/batch\n",
      "Epoch: 25/40...  Training Step: 997...  Training loss: 1.6489...  0.5172 sec/batch\n",
      "Epoch: 25/40...  Training Step: 998...  Training loss: 1.7143...  0.5272 sec/batch\n",
      "Epoch: 25/40...  Training Step: 999...  Training loss: 1.6895...  0.5137 sec/batch\n",
      "Epoch: 25/40...  Training Step: 1000...  Training loss: 1.6558...  0.5191 sec/batch\n",
      "Epoch: 26/40...  Training Step: 1001...  Training loss: 1.7062...  0.5264 sec/batch\n",
      "Epoch: 26/40...  Training Step: 1002...  Training loss: 1.6659...  0.5248 sec/batch\n",
      "Epoch: 26/40...  Training Step: 1003...  Training loss: 1.6805...  0.5161 sec/batch\n",
      "Epoch: 26/40...  Training Step: 1004...  Training loss: 1.6673...  0.5253 sec/batch\n",
      "Epoch: 26/40...  Training Step: 1005...  Training loss: 1.6870...  0.5245 sec/batch\n",
      "Epoch: 26/40...  Training Step: 1006...  Training loss: 1.6678...  0.5161 sec/batch\n",
      "Epoch: 26/40...  Training Step: 1007...  Training loss: 1.6357...  0.5234 sec/batch\n",
      "Epoch: 26/40...  Training Step: 1008...  Training loss: 1.6402...  0.5079 sec/batch\n",
      "Epoch: 26/40...  Training Step: 1009...  Training loss: 1.6748...  0.5270 sec/batch\n",
      "Epoch: 26/40...  Training Step: 1010...  Training loss: 1.6556...  0.5205 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26/40...  Training Step: 1011...  Training loss: 1.6930...  0.5120 sec/batch\n",
      "Epoch: 26/40...  Training Step: 1012...  Training loss: 1.6761...  0.5272 sec/batch\n",
      "Epoch: 26/40...  Training Step: 1013...  Training loss: 1.6569...  0.5161 sec/batch\n",
      "Epoch: 26/40...  Training Step: 1014...  Training loss: 1.6824...  0.5066 sec/batch\n",
      "Epoch: 26/40...  Training Step: 1015...  Training loss: 1.7050...  0.5045 sec/batch\n",
      "Epoch: 26/40...  Training Step: 1016...  Training loss: 1.6915...  0.5264 sec/batch\n",
      "Epoch: 26/40...  Training Step: 1017...  Training loss: 1.6414...  0.5113 sec/batch\n",
      "Epoch: 26/40...  Training Step: 1018...  Training loss: 1.6586...  0.5274 sec/batch\n",
      "Epoch: 26/40...  Training Step: 1019...  Training loss: 1.6629...  0.5262 sec/batch\n",
      "Epoch: 26/40...  Training Step: 1020...  Training loss: 1.6618...  0.5227 sec/batch\n",
      "Epoch: 26/40...  Training Step: 1021...  Training loss: 1.6572...  0.5186 sec/batch\n",
      "Epoch: 26/40...  Training Step: 1022...  Training loss: 1.6715...  0.5227 sec/batch\n",
      "Epoch: 26/40...  Training Step: 1023...  Training loss: 1.6659...  0.5216 sec/batch\n",
      "Epoch: 26/40...  Training Step: 1024...  Training loss: 1.6553...  0.5095 sec/batch\n",
      "Epoch: 26/40...  Training Step: 1025...  Training loss: 1.6235...  0.5231 sec/batch\n",
      "Epoch: 26/40...  Training Step: 1026...  Training loss: 1.6630...  0.5230 sec/batch\n",
      "Epoch: 26/40...  Training Step: 1027...  Training loss: 1.6455...  0.5133 sec/batch\n",
      "Epoch: 26/40...  Training Step: 1028...  Training loss: 1.6561...  0.5100 sec/batch\n",
      "Epoch: 26/40...  Training Step: 1029...  Training loss: 1.6616...  0.5194 sec/batch\n",
      "Epoch: 26/40...  Training Step: 1030...  Training loss: 1.6523...  0.5174 sec/batch\n",
      "Epoch: 26/40...  Training Step: 1031...  Training loss: 1.6526...  0.5271 sec/batch\n",
      "Epoch: 26/40...  Training Step: 1032...  Training loss: 1.6676...  0.5134 sec/batch\n",
      "Epoch: 26/40...  Training Step: 1033...  Training loss: 1.6711...  0.5221 sec/batch\n",
      "Epoch: 26/40...  Training Step: 1034...  Training loss: 1.6886...  0.5258 sec/batch\n",
      "Epoch: 26/40...  Training Step: 1035...  Training loss: 1.6212...  0.5301 sec/batch\n",
      "Epoch: 26/40...  Training Step: 1036...  Training loss: 1.6629...  0.5232 sec/batch\n",
      "Epoch: 26/40...  Training Step: 1037...  Training loss: 1.6395...  0.5199 sec/batch\n",
      "Epoch: 26/40...  Training Step: 1038...  Training loss: 1.6965...  0.5195 sec/batch\n",
      "Epoch: 26/40...  Training Step: 1039...  Training loss: 1.6692...  0.5068 sec/batch\n",
      "Epoch: 26/40...  Training Step: 1040...  Training loss: 1.6455...  0.5112 sec/batch\n",
      "Epoch: 27/40...  Training Step: 1041...  Training loss: 1.6735...  0.5251 sec/batch\n",
      "Epoch: 27/40...  Training Step: 1042...  Training loss: 1.6460...  0.5231 sec/batch\n",
      "Epoch: 27/40...  Training Step: 1043...  Training loss: 1.6529...  0.5157 sec/batch\n",
      "Epoch: 27/40...  Training Step: 1044...  Training loss: 1.6445...  0.5169 sec/batch\n",
      "Epoch: 27/40...  Training Step: 1045...  Training loss: 1.6621...  0.5201 sec/batch\n",
      "Epoch: 27/40...  Training Step: 1046...  Training loss: 1.6399...  0.5196 sec/batch\n",
      "Epoch: 27/40...  Training Step: 1047...  Training loss: 1.6082...  0.5150 sec/batch\n",
      "Epoch: 27/40...  Training Step: 1048...  Training loss: 1.6189...  0.5181 sec/batch\n",
      "Epoch: 27/40...  Training Step: 1049...  Training loss: 1.6235...  0.5103 sec/batch\n",
      "Epoch: 27/40...  Training Step: 1050...  Training loss: 1.6325...  0.5104 sec/batch\n",
      "Epoch: 27/40...  Training Step: 1051...  Training loss: 1.6501...  0.5170 sec/batch\n",
      "Epoch: 27/40...  Training Step: 1052...  Training loss: 1.6476...  0.5188 sec/batch\n",
      "Epoch: 27/40...  Training Step: 1053...  Training loss: 1.6228...  0.5237 sec/batch\n",
      "Epoch: 27/40...  Training Step: 1054...  Training loss: 1.6638...  0.5116 sec/batch\n",
      "Epoch: 27/40...  Training Step: 1055...  Training loss: 1.6707...  0.5171 sec/batch\n",
      "Epoch: 27/40...  Training Step: 1056...  Training loss: 1.6512...  0.5235 sec/batch\n",
      "Epoch: 27/40...  Training Step: 1057...  Training loss: 1.6122...  0.5193 sec/batch\n",
      "Epoch: 27/40...  Training Step: 1058...  Training loss: 1.6336...  0.5294 sec/batch\n",
      "Epoch: 27/40...  Training Step: 1059...  Training loss: 1.6244...  0.5175 sec/batch\n",
      "Epoch: 27/40...  Training Step: 1060...  Training loss: 1.6311...  0.5310 sec/batch\n",
      "Epoch: 27/40...  Training Step: 1061...  Training loss: 1.6320...  0.5270 sec/batch\n",
      "Epoch: 27/40...  Training Step: 1062...  Training loss: 1.6488...  0.5197 sec/batch\n",
      "Epoch: 27/40...  Training Step: 1063...  Training loss: 1.6420...  0.5175 sec/batch\n",
      "Epoch: 27/40...  Training Step: 1064...  Training loss: 1.6211...  0.5153 sec/batch\n",
      "Epoch: 27/40...  Training Step: 1065...  Training loss: 1.5993...  0.5178 sec/batch\n",
      "Epoch: 27/40...  Training Step: 1066...  Training loss: 1.6324...  0.5214 sec/batch\n",
      "Epoch: 27/40...  Training Step: 1067...  Training loss: 1.6138...  0.5107 sec/batch\n",
      "Epoch: 27/40...  Training Step: 1068...  Training loss: 1.6300...  0.5235 sec/batch\n",
      "Epoch: 27/40...  Training Step: 1069...  Training loss: 1.6417...  0.5146 sec/batch\n",
      "Epoch: 27/40...  Training Step: 1070...  Training loss: 1.6313...  0.5102 sec/batch\n",
      "Epoch: 27/40...  Training Step: 1071...  Training loss: 1.6252...  0.5262 sec/batch\n",
      "Epoch: 27/40...  Training Step: 1072...  Training loss: 1.6396...  0.5192 sec/batch\n",
      "Epoch: 27/40...  Training Step: 1073...  Training loss: 1.6455...  0.5161 sec/batch\n",
      "Epoch: 27/40...  Training Step: 1074...  Training loss: 1.6620...  0.5121 sec/batch\n",
      "Epoch: 27/40...  Training Step: 1075...  Training loss: 1.5922...  0.5267 sec/batch\n",
      "Epoch: 27/40...  Training Step: 1076...  Training loss: 1.6389...  0.5201 sec/batch\n",
      "Epoch: 27/40...  Training Step: 1077...  Training loss: 1.6090...  0.5007 sec/batch\n",
      "Epoch: 27/40...  Training Step: 1078...  Training loss: 1.6723...  0.5156 sec/batch\n",
      "Epoch: 27/40...  Training Step: 1079...  Training loss: 1.6475...  0.5268 sec/batch\n",
      "Epoch: 27/40...  Training Step: 1080...  Training loss: 1.6148...  0.5077 sec/batch\n",
      "Epoch: 28/40...  Training Step: 1081...  Training loss: 1.6612...  0.5179 sec/batch\n",
      "Epoch: 28/40...  Training Step: 1082...  Training loss: 1.6156...  0.5063 sec/batch\n",
      "Epoch: 28/40...  Training Step: 1083...  Training loss: 1.6218...  0.5077 sec/batch\n",
      "Epoch: 28/40...  Training Step: 1084...  Training loss: 1.6144...  0.5109 sec/batch\n",
      "Epoch: 28/40...  Training Step: 1085...  Training loss: 1.6397...  0.5250 sec/batch\n",
      "Epoch: 28/40...  Training Step: 1086...  Training loss: 1.6199...  0.5186 sec/batch\n",
      "Epoch: 28/40...  Training Step: 1087...  Training loss: 1.5839...  0.5161 sec/batch\n",
      "Epoch: 28/40...  Training Step: 1088...  Training loss: 1.5949...  0.5155 sec/batch\n",
      "Epoch: 28/40...  Training Step: 1089...  Training loss: 1.6059...  0.5119 sec/batch\n",
      "Epoch: 28/40...  Training Step: 1090...  Training loss: 1.6056...  0.5239 sec/batch\n",
      "Epoch: 28/40...  Training Step: 1091...  Training loss: 1.6230...  0.5186 sec/batch\n",
      "Epoch: 28/40...  Training Step: 1092...  Training loss: 1.6178...  0.5281 sec/batch\n",
      "Epoch: 28/40...  Training Step: 1093...  Training loss: 1.5932...  0.5208 sec/batch\n",
      "Epoch: 28/40...  Training Step: 1094...  Training loss: 1.6327...  0.5127 sec/batch\n",
      "Epoch: 28/40...  Training Step: 1095...  Training loss: 1.6382...  0.5208 sec/batch\n",
      "Epoch: 28/40...  Training Step: 1096...  Training loss: 1.6228...  0.5205 sec/batch\n",
      "Epoch: 28/40...  Training Step: 1097...  Training loss: 1.5873...  0.5135 sec/batch\n",
      "Epoch: 28/40...  Training Step: 1098...  Training loss: 1.5996...  0.5193 sec/batch\n",
      "Epoch: 28/40...  Training Step: 1099...  Training loss: 1.5952...  0.5153 sec/batch\n",
      "Epoch: 28/40...  Training Step: 1100...  Training loss: 1.5948...  0.5115 sec/batch\n",
      "Epoch: 28/40...  Training Step: 1101...  Training loss: 1.6117...  0.5221 sec/batch\n",
      "Epoch: 28/40...  Training Step: 1102...  Training loss: 1.6275...  0.5104 sec/batch\n",
      "Epoch: 28/40...  Training Step: 1103...  Training loss: 1.6037...  0.5218 sec/batch\n",
      "Epoch: 28/40...  Training Step: 1104...  Training loss: 1.5978...  0.5099 sec/batch\n",
      "Epoch: 28/40...  Training Step: 1105...  Training loss: 1.5640...  0.5249 sec/batch\n",
      "Epoch: 28/40...  Training Step: 1106...  Training loss: 1.6040...  0.5056 sec/batch\n",
      "Epoch: 28/40...  Training Step: 1107...  Training loss: 1.5838...  0.5114 sec/batch\n",
      "Epoch: 28/40...  Training Step: 1108...  Training loss: 1.6012...  0.5028 sec/batch\n",
      "Epoch: 28/40...  Training Step: 1109...  Training loss: 1.6062...  0.5060 sec/batch\n",
      "Epoch: 28/40...  Training Step: 1110...  Training loss: 1.5976...  0.5177 sec/batch\n",
      "Epoch: 28/40...  Training Step: 1111...  Training loss: 1.5942...  0.5171 sec/batch\n",
      "Epoch: 28/40...  Training Step: 1112...  Training loss: 1.6018...  0.5177 sec/batch\n",
      "Epoch: 28/40...  Training Step: 1113...  Training loss: 1.6133...  0.5106 sec/batch\n",
      "Epoch: 28/40...  Training Step: 1114...  Training loss: 1.6273...  0.5081 sec/batch\n",
      "Epoch: 28/40...  Training Step: 1115...  Training loss: 1.5677...  0.5213 sec/batch\n",
      "Epoch: 28/40...  Training Step: 1116...  Training loss: 1.6111...  0.5162 sec/batch\n",
      "Epoch: 28/40...  Training Step: 1117...  Training loss: 1.5859...  0.5241 sec/batch\n",
      "Epoch: 28/40...  Training Step: 1118...  Training loss: 1.6481...  0.5117 sec/batch\n",
      "Epoch: 28/40...  Training Step: 1119...  Training loss: 1.6245...  0.5230 sec/batch\n",
      "Epoch: 28/40...  Training Step: 1120...  Training loss: 1.5856...  0.5020 sec/batch\n",
      "Epoch: 29/40...  Training Step: 1121...  Training loss: 1.6489...  0.5174 sec/batch\n",
      "Epoch: 29/40...  Training Step: 1122...  Training loss: 1.6044...  0.5215 sec/batch\n",
      "Epoch: 29/40...  Training Step: 1123...  Training loss: 1.6149...  0.5234 sec/batch\n",
      "Epoch: 29/40...  Training Step: 1124...  Training loss: 1.5974...  0.5186 sec/batch\n",
      "Epoch: 29/40...  Training Step: 1125...  Training loss: 1.6270...  0.5242 sec/batch\n",
      "Epoch: 29/40...  Training Step: 1126...  Training loss: 1.5975...  0.5266 sec/batch\n",
      "Epoch: 29/40...  Training Step: 1127...  Training loss: 1.5581...  0.5033 sec/batch\n",
      "Epoch: 29/40...  Training Step: 1128...  Training loss: 1.5659...  0.5319 sec/batch\n",
      "Epoch: 29/40...  Training Step: 1129...  Training loss: 1.5813...  0.5220 sec/batch\n",
      "Epoch: 29/40...  Training Step: 1130...  Training loss: 1.5922...  0.5226 sec/batch\n",
      "Epoch: 29/40...  Training Step: 1131...  Training loss: 1.6078...  0.5151 sec/batch\n",
      "Epoch: 29/40...  Training Step: 1132...  Training loss: 1.5901...  0.5043 sec/batch\n",
      "Epoch: 29/40...  Training Step: 1133...  Training loss: 1.5771...  0.5095 sec/batch\n",
      "Epoch: 29/40...  Training Step: 1134...  Training loss: 1.6046...  0.5247 sec/batch\n",
      "Epoch: 29/40...  Training Step: 1135...  Training loss: 1.6275...  0.5188 sec/batch\n",
      "Epoch: 29/40...  Training Step: 1136...  Training loss: 1.5991...  0.5151 sec/batch\n",
      "Epoch: 29/40...  Training Step: 1137...  Training loss: 1.5679...  0.5205 sec/batch\n",
      "Epoch: 29/40...  Training Step: 1138...  Training loss: 1.5781...  0.5093 sec/batch\n",
      "Epoch: 29/40...  Training Step: 1139...  Training loss: 1.5737...  0.5088 sec/batch\n",
      "Epoch: 29/40...  Training Step: 1140...  Training loss: 1.5767...  0.5128 sec/batch\n",
      "Epoch: 29/40...  Training Step: 1141...  Training loss: 1.5854...  0.5219 sec/batch\n",
      "Epoch: 29/40...  Training Step: 1142...  Training loss: 1.5983...  0.5117 sec/batch\n",
      "Epoch: 29/40...  Training Step: 1143...  Training loss: 1.5872...  0.5057 sec/batch\n",
      "Epoch: 29/40...  Training Step: 1144...  Training loss: 1.5817...  0.5201 sec/batch\n",
      "Epoch: 29/40...  Training Step: 1145...  Training loss: 1.5429...  0.5207 sec/batch\n",
      "Epoch: 29/40...  Training Step: 1146...  Training loss: 1.5863...  0.5216 sec/batch\n",
      "Epoch: 29/40...  Training Step: 1147...  Training loss: 1.5656...  0.5211 sec/batch\n",
      "Epoch: 29/40...  Training Step: 1148...  Training loss: 1.5758...  0.5134 sec/batch\n",
      "Epoch: 29/40...  Training Step: 1149...  Training loss: 1.5862...  0.5041 sec/batch\n",
      "Epoch: 29/40...  Training Step: 1150...  Training loss: 1.5884...  0.5089 sec/batch\n",
      "Epoch: 29/40...  Training Step: 1151...  Training loss: 1.5742...  0.5206 sec/batch\n",
      "Epoch: 29/40...  Training Step: 1152...  Training loss: 1.5911...  0.5214 sec/batch\n",
      "Epoch: 29/40...  Training Step: 1153...  Training loss: 1.5936...  0.5265 sec/batch\n",
      "Epoch: 29/40...  Training Step: 1154...  Training loss: 1.6105...  0.5063 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29/40...  Training Step: 1155...  Training loss: 1.5440...  0.5157 sec/batch\n",
      "Epoch: 29/40...  Training Step: 1156...  Training loss: 1.5911...  0.5141 sec/batch\n",
      "Epoch: 29/40...  Training Step: 1157...  Training loss: 1.5678...  0.5207 sec/batch\n",
      "Epoch: 29/40...  Training Step: 1158...  Training loss: 1.6271...  0.5113 sec/batch\n",
      "Epoch: 29/40...  Training Step: 1159...  Training loss: 1.5963...  0.5177 sec/batch\n",
      "Epoch: 29/40...  Training Step: 1160...  Training loss: 1.5603...  0.5230 sec/batch\n",
      "Epoch: 30/40...  Training Step: 1161...  Training loss: 1.6321...  0.5121 sec/batch\n",
      "Epoch: 30/40...  Training Step: 1162...  Training loss: 1.5823...  0.5233 sec/batch\n",
      "Epoch: 30/40...  Training Step: 1163...  Training loss: 1.5844...  0.5121 sec/batch\n",
      "Epoch: 30/40...  Training Step: 1164...  Training loss: 1.5849...  0.5135 sec/batch\n",
      "Epoch: 30/40...  Training Step: 1165...  Training loss: 1.5986...  0.5235 sec/batch\n",
      "Epoch: 30/40...  Training Step: 1166...  Training loss: 1.5836...  0.5137 sec/batch\n",
      "Epoch: 30/40...  Training Step: 1167...  Training loss: 1.5339...  0.5141 sec/batch\n",
      "Epoch: 30/40...  Training Step: 1168...  Training loss: 1.5418...  0.5201 sec/batch\n",
      "Epoch: 30/40...  Training Step: 1169...  Training loss: 1.5637...  0.5104 sec/batch\n",
      "Epoch: 30/40...  Training Step: 1170...  Training loss: 1.5652...  0.5129 sec/batch\n",
      "Epoch: 30/40...  Training Step: 1171...  Training loss: 1.6023...  0.5124 sec/batch\n",
      "Epoch: 30/40...  Training Step: 1172...  Training loss: 1.5765...  0.5107 sec/batch\n",
      "Epoch: 30/40...  Training Step: 1173...  Training loss: 1.5651...  0.5191 sec/batch\n",
      "Epoch: 30/40...  Training Step: 1174...  Training loss: 1.5962...  0.5131 sec/batch\n",
      "Epoch: 30/40...  Training Step: 1175...  Training loss: 1.6196...  0.5164 sec/batch\n",
      "Epoch: 30/40...  Training Step: 1176...  Training loss: 1.5904...  0.5253 sec/batch\n",
      "Epoch: 30/40...  Training Step: 1177...  Training loss: 1.5540...  0.5173 sec/batch\n",
      "Epoch: 30/40...  Training Step: 1178...  Training loss: 1.5704...  0.5268 sec/batch\n",
      "Epoch: 30/40...  Training Step: 1179...  Training loss: 1.5674...  0.5275 sec/batch\n",
      "Epoch: 30/40...  Training Step: 1180...  Training loss: 1.5711...  0.5208 sec/batch\n",
      "Epoch: 30/40...  Training Step: 1181...  Training loss: 1.5753...  0.5258 sec/batch\n",
      "Epoch: 30/40...  Training Step: 1182...  Training loss: 1.5914...  0.5223 sec/batch\n",
      "Epoch: 30/40...  Training Step: 1183...  Training loss: 1.5629...  0.5085 sec/batch\n",
      "Epoch: 30/40...  Training Step: 1184...  Training loss: 1.5620...  0.5028 sec/batch\n",
      "Epoch: 30/40...  Training Step: 1185...  Training loss: 1.5303...  0.5215 sec/batch\n",
      "Epoch: 30/40...  Training Step: 1186...  Training loss: 1.5646...  0.5180 sec/batch\n",
      "Epoch: 30/40...  Training Step: 1187...  Training loss: 1.5506...  0.5223 sec/batch\n",
      "Epoch: 30/40...  Training Step: 1188...  Training loss: 1.5601...  0.5120 sec/batch\n",
      "Epoch: 30/40...  Training Step: 1189...  Training loss: 1.5679...  0.5147 sec/batch\n",
      "Epoch: 30/40...  Training Step: 1190...  Training loss: 1.5688...  0.5256 sec/batch\n",
      "Epoch: 30/40...  Training Step: 1191...  Training loss: 1.5627...  0.5224 sec/batch\n",
      "Epoch: 30/40...  Training Step: 1192...  Training loss: 1.5699...  0.5145 sec/batch\n",
      "Epoch: 30/40...  Training Step: 1193...  Training loss: 1.5729...  0.5240 sec/batch\n",
      "Epoch: 30/40...  Training Step: 1194...  Training loss: 1.5939...  0.5090 sec/batch\n",
      "Epoch: 30/40...  Training Step: 1195...  Training loss: 1.5294...  0.5235 sec/batch\n",
      "Epoch: 30/40...  Training Step: 1196...  Training loss: 1.5762...  0.5101 sec/batch\n",
      "Epoch: 30/40...  Training Step: 1197...  Training loss: 1.5525...  0.5210 sec/batch\n",
      "Epoch: 30/40...  Training Step: 1198...  Training loss: 1.6110...  0.5187 sec/batch\n",
      "Epoch: 30/40...  Training Step: 1199...  Training loss: 1.5797...  0.5172 sec/batch\n",
      "Epoch: 30/40...  Training Step: 1200...  Training loss: 1.5408...  0.5153 sec/batch\n",
      "Epoch: 31/40...  Training Step: 1201...  Training loss: 1.6031...  0.5241 sec/batch\n",
      "Epoch: 31/40...  Training Step: 1202...  Training loss: 1.5648...  0.5211 sec/batch\n",
      "Epoch: 31/40...  Training Step: 1203...  Training loss: 1.5697...  0.5237 sec/batch\n",
      "Epoch: 31/40...  Training Step: 1204...  Training loss: 1.5629...  0.5166 sec/batch\n",
      "Epoch: 31/40...  Training Step: 1205...  Training loss: 1.5761...  0.5280 sec/batch\n",
      "Epoch: 31/40...  Training Step: 1206...  Training loss: 1.5603...  0.5185 sec/batch\n",
      "Epoch: 31/40...  Training Step: 1207...  Training loss: 1.5218...  0.5222 sec/batch\n",
      "Epoch: 31/40...  Training Step: 1208...  Training loss: 1.5214...  0.5117 sec/batch\n",
      "Epoch: 31/40...  Training Step: 1209...  Training loss: 1.5437...  0.5277 sec/batch\n",
      "Epoch: 31/40...  Training Step: 1210...  Training loss: 1.5435...  0.5195 sec/batch\n",
      "Epoch: 31/40...  Training Step: 1211...  Training loss: 1.5661...  0.5170 sec/batch\n",
      "Epoch: 31/40...  Training Step: 1212...  Training loss: 1.5525...  0.5045 sec/batch\n",
      "Epoch: 31/40...  Training Step: 1213...  Training loss: 1.5411...  0.5144 sec/batch\n",
      "Epoch: 31/40...  Training Step: 1214...  Training loss: 1.5659...  0.5161 sec/batch\n",
      "Epoch: 31/40...  Training Step: 1215...  Training loss: 1.5838...  0.5049 sec/batch\n",
      "Epoch: 31/40...  Training Step: 1216...  Training loss: 1.5703...  0.5185 sec/batch\n",
      "Epoch: 31/40...  Training Step: 1217...  Training loss: 1.5276...  0.5255 sec/batch\n",
      "Epoch: 31/40...  Training Step: 1218...  Training loss: 1.5436...  0.5216 sec/batch\n",
      "Epoch: 31/40...  Training Step: 1219...  Training loss: 1.5421...  0.5218 sec/batch\n",
      "Epoch: 31/40...  Training Step: 1220...  Training loss: 1.5397...  0.5124 sec/batch\n",
      "Epoch: 31/40...  Training Step: 1221...  Training loss: 1.5523...  0.5119 sec/batch\n",
      "Epoch: 31/40...  Training Step: 1222...  Training loss: 1.5619...  0.5257 sec/batch\n",
      "Epoch: 31/40...  Training Step: 1223...  Training loss: 1.5470...  0.5192 sec/batch\n",
      "Epoch: 31/40...  Training Step: 1224...  Training loss: 1.5465...  0.5126 sec/batch\n",
      "Epoch: 31/40...  Training Step: 1225...  Training loss: 1.5112...  0.5253 sec/batch\n",
      "Epoch: 31/40...  Training Step: 1226...  Training loss: 1.5433...  0.5154 sec/batch\n",
      "Epoch: 31/40...  Training Step: 1227...  Training loss: 1.5210...  0.5140 sec/batch\n",
      "Epoch: 31/40...  Training Step: 1228...  Training loss: 1.5386...  0.5198 sec/batch\n",
      "Epoch: 31/40...  Training Step: 1229...  Training loss: 1.5451...  0.5285 sec/batch\n",
      "Epoch: 31/40...  Training Step: 1230...  Training loss: 1.5356...  0.5177 sec/batch\n",
      "Epoch: 31/40...  Training Step: 1231...  Training loss: 1.5345...  0.5234 sec/batch\n",
      "Epoch: 31/40...  Training Step: 1232...  Training loss: 1.5476...  0.5169 sec/batch\n",
      "Epoch: 31/40...  Training Step: 1233...  Training loss: 1.5547...  0.5206 sec/batch\n",
      "Epoch: 31/40...  Training Step: 1234...  Training loss: 1.5733...  0.5213 sec/batch\n",
      "Epoch: 31/40...  Training Step: 1235...  Training loss: 1.5049...  0.5283 sec/batch\n",
      "Epoch: 31/40...  Training Step: 1236...  Training loss: 1.5619...  0.5270 sec/batch\n",
      "Epoch: 31/40...  Training Step: 1237...  Training loss: 1.5362...  0.5279 sec/batch\n",
      "Epoch: 31/40...  Training Step: 1238...  Training loss: 1.5836...  0.5080 sec/batch\n",
      "Epoch: 31/40...  Training Step: 1239...  Training loss: 1.5656...  0.5241 sec/batch\n",
      "Epoch: 31/40...  Training Step: 1240...  Training loss: 1.5277...  0.5194 sec/batch\n",
      "Epoch: 32/40...  Training Step: 1241...  Training loss: 1.5908...  0.5156 sec/batch\n",
      "Epoch: 32/40...  Training Step: 1242...  Training loss: 1.5601...  0.5154 sec/batch\n",
      "Epoch: 32/40...  Training Step: 1243...  Training loss: 1.5514...  0.5189 sec/batch\n",
      "Epoch: 32/40...  Training Step: 1244...  Training loss: 1.5564...  0.5196 sec/batch\n",
      "Epoch: 32/40...  Training Step: 1245...  Training loss: 1.5569...  0.5109 sec/batch\n",
      "Epoch: 32/40...  Training Step: 1246...  Training loss: 1.5487...  0.5105 sec/batch\n",
      "Epoch: 32/40...  Training Step: 1247...  Training loss: 1.4967...  0.5156 sec/batch\n",
      "Epoch: 32/40...  Training Step: 1248...  Training loss: 1.5123...  0.5153 sec/batch\n",
      "Epoch: 32/40...  Training Step: 1249...  Training loss: 1.5353...  0.5119 sec/batch\n",
      "Epoch: 32/40...  Training Step: 1250...  Training loss: 1.5299...  0.5095 sec/batch\n",
      "Epoch: 32/40...  Training Step: 1251...  Training loss: 1.5630...  0.5171 sec/batch\n",
      "Epoch: 32/40...  Training Step: 1252...  Training loss: 1.5523...  0.5131 sec/batch\n",
      "Epoch: 32/40...  Training Step: 1253...  Training loss: 1.5256...  0.5258 sec/batch\n",
      "Epoch: 32/40...  Training Step: 1254...  Training loss: 1.5519...  0.5222 sec/batch\n",
      "Epoch: 32/40...  Training Step: 1255...  Training loss: 1.5676...  0.5193 sec/batch\n",
      "Epoch: 32/40...  Training Step: 1256...  Training loss: 1.5574...  0.5192 sec/batch\n",
      "Epoch: 32/40...  Training Step: 1257...  Training loss: 1.5236...  0.5178 sec/batch\n",
      "Epoch: 32/40...  Training Step: 1258...  Training loss: 1.5346...  0.5252 sec/batch\n",
      "Epoch: 32/40...  Training Step: 1259...  Training loss: 1.5237...  0.5018 sec/batch\n",
      "Epoch: 32/40...  Training Step: 1260...  Training loss: 1.5262...  0.5159 sec/batch\n",
      "Epoch: 32/40...  Training Step: 1261...  Training loss: 1.5294...  0.5164 sec/batch\n",
      "Epoch: 32/40...  Training Step: 1262...  Training loss: 1.5427...  0.5148 sec/batch\n",
      "Epoch: 32/40...  Training Step: 1263...  Training loss: 1.5316...  0.5174 sec/batch\n",
      "Epoch: 32/40...  Training Step: 1264...  Training loss: 1.5235...  0.5269 sec/batch\n",
      "Epoch: 32/40...  Training Step: 1265...  Training loss: 1.4993...  0.5208 sec/batch\n",
      "Epoch: 32/40...  Training Step: 1266...  Training loss: 1.5236...  0.5108 sec/batch\n",
      "Epoch: 32/40...  Training Step: 1267...  Training loss: 1.5129...  0.5172 sec/batch\n",
      "Epoch: 32/40...  Training Step: 1268...  Training loss: 1.5294...  0.5095 sec/batch\n",
      "Epoch: 32/40...  Training Step: 1269...  Training loss: 1.5372...  0.5061 sec/batch\n",
      "Epoch: 32/40...  Training Step: 1270...  Training loss: 1.5223...  0.5214 sec/batch\n",
      "Epoch: 32/40...  Training Step: 1271...  Training loss: 1.5277...  0.5179 sec/batch\n",
      "Epoch: 32/40...  Training Step: 1272...  Training loss: 1.5411...  0.5178 sec/batch\n",
      "Epoch: 32/40...  Training Step: 1273...  Training loss: 1.5438...  0.5246 sec/batch\n",
      "Epoch: 32/40...  Training Step: 1274...  Training loss: 1.5705...  0.5139 sec/batch\n",
      "Epoch: 32/40...  Training Step: 1275...  Training loss: 1.4936...  0.5123 sec/batch\n",
      "Epoch: 32/40...  Training Step: 1276...  Training loss: 1.5481...  0.5248 sec/batch\n",
      "Epoch: 32/40...  Training Step: 1277...  Training loss: 1.5157...  0.5234 sec/batch\n",
      "Epoch: 32/40...  Training Step: 1278...  Training loss: 1.5747...  0.5159 sec/batch\n",
      "Epoch: 32/40...  Training Step: 1279...  Training loss: 1.5449...  0.5073 sec/batch\n",
      "Epoch: 32/40...  Training Step: 1280...  Training loss: 1.5092...  0.5184 sec/batch\n",
      "Epoch: 33/40...  Training Step: 1281...  Training loss: 1.5641...  0.5190 sec/batch\n",
      "Epoch: 33/40...  Training Step: 1282...  Training loss: 1.5459...  0.5171 sec/batch\n",
      "Epoch: 33/40...  Training Step: 1283...  Training loss: 1.5388...  0.5316 sec/batch\n",
      "Epoch: 33/40...  Training Step: 1284...  Training loss: 1.5418...  0.5203 sec/batch\n",
      "Epoch: 33/40...  Training Step: 1285...  Training loss: 1.5408...  0.5195 sec/batch\n",
      "Epoch: 33/40...  Training Step: 1286...  Training loss: 1.5289...  0.5227 sec/batch\n",
      "Epoch: 33/40...  Training Step: 1287...  Training loss: 1.4931...  0.5237 sec/batch\n",
      "Epoch: 33/40...  Training Step: 1288...  Training loss: 1.4920...  0.5040 sec/batch\n",
      "Epoch: 33/40...  Training Step: 1289...  Training loss: 1.5199...  0.5148 sec/batch\n",
      "Epoch: 33/40...  Training Step: 1290...  Training loss: 1.5142...  0.5184 sec/batch\n",
      "Epoch: 33/40...  Training Step: 1291...  Training loss: 1.5337...  0.5166 sec/batch\n",
      "Epoch: 33/40...  Training Step: 1292...  Training loss: 1.5301...  0.5188 sec/batch\n",
      "Epoch: 33/40...  Training Step: 1293...  Training loss: 1.5128...  0.5228 sec/batch\n",
      "Epoch: 33/40...  Training Step: 1294...  Training loss: 1.5462...  0.5050 sec/batch\n",
      "Epoch: 33/40...  Training Step: 1295...  Training loss: 1.5628...  0.5164 sec/batch\n",
      "Epoch: 33/40...  Training Step: 1296...  Training loss: 1.5461...  0.5106 sec/batch\n",
      "Epoch: 33/40...  Training Step: 1297...  Training loss: 1.5024...  0.5141 sec/batch\n",
      "Epoch: 33/40...  Training Step: 1298...  Training loss: 1.5183...  0.5148 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33/40...  Training Step: 1299...  Training loss: 1.5017...  0.5117 sec/batch\n",
      "Epoch: 33/40...  Training Step: 1300...  Training loss: 1.5149...  0.5199 sec/batch\n",
      "Epoch: 33/40...  Training Step: 1301...  Training loss: 1.5235...  0.5206 sec/batch\n",
      "Epoch: 33/40...  Training Step: 1302...  Training loss: 1.5333...  0.5145 sec/batch\n",
      "Epoch: 33/40...  Training Step: 1303...  Training loss: 1.5142...  0.5055 sec/batch\n",
      "Epoch: 33/40...  Training Step: 1304...  Training loss: 1.5138...  0.5230 sec/batch\n",
      "Epoch: 33/40...  Training Step: 1305...  Training loss: 1.4775...  0.5094 sec/batch\n",
      "Epoch: 33/40...  Training Step: 1306...  Training loss: 1.5145...  0.5248 sec/batch\n",
      "Epoch: 33/40...  Training Step: 1307...  Training loss: 1.4955...  0.5162 sec/batch\n",
      "Epoch: 33/40...  Training Step: 1308...  Training loss: 1.5073...  0.5168 sec/batch\n",
      "Epoch: 33/40...  Training Step: 1309...  Training loss: 1.5154...  0.5206 sec/batch\n",
      "Epoch: 33/40...  Training Step: 1310...  Training loss: 1.5190...  0.5216 sec/batch\n",
      "Epoch: 33/40...  Training Step: 1311...  Training loss: 1.5064...  0.5125 sec/batch\n",
      "Epoch: 33/40...  Training Step: 1312...  Training loss: 1.5218...  0.5150 sec/batch\n",
      "Epoch: 33/40...  Training Step: 1313...  Training loss: 1.5294...  0.5214 sec/batch\n",
      "Epoch: 33/40...  Training Step: 1314...  Training loss: 1.5555...  0.5080 sec/batch\n",
      "Epoch: 33/40...  Training Step: 1315...  Training loss: 1.4916...  0.5106 sec/batch\n",
      "Epoch: 33/40...  Training Step: 1316...  Training loss: 1.5280...  0.5170 sec/batch\n",
      "Epoch: 33/40...  Training Step: 1317...  Training loss: 1.4951...  0.5132 sec/batch\n",
      "Epoch: 33/40...  Training Step: 1318...  Training loss: 1.5676...  0.5050 sec/batch\n",
      "Epoch: 33/40...  Training Step: 1319...  Training loss: 1.5417...  0.5289 sec/batch\n",
      "Epoch: 33/40...  Training Step: 1320...  Training loss: 1.5010...  0.5264 sec/batch\n",
      "Epoch: 34/40...  Training Step: 1321...  Training loss: 1.5637...  0.5107 sec/batch\n",
      "Epoch: 34/40...  Training Step: 1322...  Training loss: 1.5303...  0.5073 sec/batch\n",
      "Epoch: 34/40...  Training Step: 1323...  Training loss: 1.5408...  0.5219 sec/batch\n",
      "Epoch: 34/40...  Training Step: 1324...  Training loss: 1.5083...  0.5070 sec/batch\n",
      "Epoch: 34/40...  Training Step: 1325...  Training loss: 1.5426...  0.5175 sec/batch\n",
      "Epoch: 34/40...  Training Step: 1326...  Training loss: 1.5175...  0.5074 sec/batch\n",
      "Epoch: 34/40...  Training Step: 1327...  Training loss: 1.4826...  0.5145 sec/batch\n",
      "Epoch: 34/40...  Training Step: 1328...  Training loss: 1.5030...  0.5059 sec/batch\n",
      "Epoch: 34/40...  Training Step: 1329...  Training loss: 1.5049...  0.5129 sec/batch\n",
      "Epoch: 34/40...  Training Step: 1330...  Training loss: 1.5067...  0.5051 sec/batch\n",
      "Epoch: 34/40...  Training Step: 1331...  Training loss: 1.5189...  0.5230 sec/batch\n",
      "Epoch: 34/40...  Training Step: 1332...  Training loss: 1.5202...  0.5217 sec/batch\n",
      "Epoch: 34/40...  Training Step: 1333...  Training loss: 1.4966...  0.5225 sec/batch\n",
      "Epoch: 34/40...  Training Step: 1334...  Training loss: 1.5354...  0.5096 sec/batch\n",
      "Epoch: 34/40...  Training Step: 1335...  Training loss: 1.5499...  0.5190 sec/batch\n",
      "Epoch: 34/40...  Training Step: 1336...  Training loss: 1.5295...  0.5120 sec/batch\n",
      "Epoch: 34/40...  Training Step: 1337...  Training loss: 1.4940...  0.5229 sec/batch\n",
      "Epoch: 34/40...  Training Step: 1338...  Training loss: 1.5166...  0.5268 sec/batch\n",
      "Epoch: 34/40...  Training Step: 1339...  Training loss: 1.5001...  0.5200 sec/batch\n",
      "Epoch: 34/40...  Training Step: 1340...  Training loss: 1.5000...  0.5199 sec/batch\n",
      "Epoch: 34/40...  Training Step: 1341...  Training loss: 1.5060...  0.5115 sec/batch\n",
      "Epoch: 34/40...  Training Step: 1342...  Training loss: 1.5253...  0.5243 sec/batch\n",
      "Epoch: 34/40...  Training Step: 1343...  Training loss: 1.5101...  0.5263 sec/batch\n",
      "Epoch: 34/40...  Training Step: 1344...  Training loss: 1.5012...  0.5254 sec/batch\n",
      "Epoch: 34/40...  Training Step: 1345...  Training loss: 1.4709...  0.5220 sec/batch\n",
      "Epoch: 34/40...  Training Step: 1346...  Training loss: 1.5041...  0.5150 sec/batch\n",
      "Epoch: 34/40...  Training Step: 1347...  Training loss: 1.4927...  0.5229 sec/batch\n",
      "Epoch: 34/40...  Training Step: 1348...  Training loss: 1.4942...  0.5073 sec/batch\n",
      "Epoch: 34/40...  Training Step: 1349...  Training loss: 1.4995...  0.5186 sec/batch\n",
      "Epoch: 34/40...  Training Step: 1350...  Training loss: 1.4944...  0.5232 sec/batch\n",
      "Epoch: 34/40...  Training Step: 1351...  Training loss: 1.4991...  0.5066 sec/batch\n",
      "Epoch: 34/40...  Training Step: 1352...  Training loss: 1.5093...  0.5163 sec/batch\n",
      "Epoch: 34/40...  Training Step: 1353...  Training loss: 1.5153...  0.5266 sec/batch\n",
      "Epoch: 34/40...  Training Step: 1354...  Training loss: 1.5289...  0.5240 sec/batch\n",
      "Epoch: 34/40...  Training Step: 1355...  Training loss: 1.4619...  0.5226 sec/batch\n",
      "Epoch: 34/40...  Training Step: 1356...  Training loss: 1.5224...  0.5256 sec/batch\n",
      "Epoch: 34/40...  Training Step: 1357...  Training loss: 1.4903...  0.5239 sec/batch\n",
      "Epoch: 34/40...  Training Step: 1358...  Training loss: 1.5458...  0.5169 sec/batch\n",
      "Epoch: 34/40...  Training Step: 1359...  Training loss: 1.5216...  0.5132 sec/batch\n",
      "Epoch: 34/40...  Training Step: 1360...  Training loss: 1.4940...  0.5148 sec/batch\n",
      "Epoch: 35/40...  Training Step: 1361...  Training loss: 1.5505...  0.5150 sec/batch\n",
      "Epoch: 35/40...  Training Step: 1362...  Training loss: 1.5152...  0.5050 sec/batch\n",
      "Epoch: 35/40...  Training Step: 1363...  Training loss: 1.5322...  0.5146 sec/batch\n",
      "Epoch: 35/40...  Training Step: 1364...  Training loss: 1.5007...  0.5325 sec/batch\n",
      "Epoch: 35/40...  Training Step: 1365...  Training loss: 1.5114...  0.5241 sec/batch\n",
      "Epoch: 35/40...  Training Step: 1366...  Training loss: 1.5066...  0.5246 sec/batch\n",
      "Epoch: 35/40...  Training Step: 1367...  Training loss: 1.4663...  0.5215 sec/batch\n",
      "Epoch: 35/40...  Training Step: 1368...  Training loss: 1.4808...  0.5214 sec/batch\n",
      "Epoch: 35/40...  Training Step: 1369...  Training loss: 1.4868...  0.5191 sec/batch\n",
      "Epoch: 35/40...  Training Step: 1370...  Training loss: 1.4914...  0.5105 sec/batch\n",
      "Epoch: 35/40...  Training Step: 1371...  Training loss: 1.5163...  0.5198 sec/batch\n",
      "Epoch: 35/40...  Training Step: 1372...  Training loss: 1.4974...  0.5233 sec/batch\n",
      "Epoch: 35/40...  Training Step: 1373...  Training loss: 1.4739...  0.5224 sec/batch\n",
      "Epoch: 35/40...  Training Step: 1374...  Training loss: 1.5209...  0.5201 sec/batch\n",
      "Epoch: 35/40...  Training Step: 1375...  Training loss: 1.5286...  0.5255 sec/batch\n",
      "Epoch: 35/40...  Training Step: 1376...  Training loss: 1.5190...  0.5055 sec/batch\n",
      "Epoch: 35/40...  Training Step: 1377...  Training loss: 1.4759...  0.5017 sec/batch\n",
      "Epoch: 35/40...  Training Step: 1378...  Training loss: 1.4886...  0.5247 sec/batch\n",
      "Epoch: 35/40...  Training Step: 1379...  Training loss: 1.4887...  0.5190 sec/batch\n",
      "Epoch: 35/40...  Training Step: 1380...  Training loss: 1.4932...  0.5148 sec/batch\n",
      "Epoch: 35/40...  Training Step: 1381...  Training loss: 1.4896...  0.5228 sec/batch\n",
      "Epoch: 35/40...  Training Step: 1382...  Training loss: 1.5040...  0.5086 sec/batch\n",
      "Epoch: 35/40...  Training Step: 1383...  Training loss: 1.4905...  0.5091 sec/batch\n",
      "Epoch: 35/40...  Training Step: 1384...  Training loss: 1.4817...  0.5224 sec/batch\n",
      "Epoch: 35/40...  Training Step: 1385...  Training loss: 1.4574...  0.5218 sec/batch\n",
      "Epoch: 35/40...  Training Step: 1386...  Training loss: 1.4879...  0.5203 sec/batch\n",
      "Epoch: 35/40...  Training Step: 1387...  Training loss: 1.4779...  0.5077 sec/batch\n",
      "Epoch: 35/40...  Training Step: 1388...  Training loss: 1.4772...  0.5063 sec/batch\n",
      "Epoch: 35/40...  Training Step: 1389...  Training loss: 1.4846...  0.5040 sec/batch\n",
      "Epoch: 35/40...  Training Step: 1390...  Training loss: 1.4832...  0.5082 sec/batch\n",
      "Epoch: 35/40...  Training Step: 1391...  Training loss: 1.4784...  0.5165 sec/batch\n",
      "Epoch: 35/40...  Training Step: 1392...  Training loss: 1.4851...  0.5140 sec/batch\n",
      "Epoch: 35/40...  Training Step: 1393...  Training loss: 1.5021...  0.5173 sec/batch\n",
      "Epoch: 35/40...  Training Step: 1394...  Training loss: 1.5175...  0.5101 sec/batch\n",
      "Epoch: 35/40...  Training Step: 1395...  Training loss: 1.4478...  0.5092 sec/batch\n",
      "Epoch: 35/40...  Training Step: 1396...  Training loss: 1.5033...  0.5037 sec/batch\n",
      "Epoch: 35/40...  Training Step: 1397...  Training loss: 1.4705...  0.5105 sec/batch\n",
      "Epoch: 35/40...  Training Step: 1398...  Training loss: 1.5326...  0.5190 sec/batch\n",
      "Epoch: 35/40...  Training Step: 1399...  Training loss: 1.5125...  0.5277 sec/batch\n",
      "Epoch: 35/40...  Training Step: 1400...  Training loss: 1.4676...  0.5076 sec/batch\n",
      "Epoch: 36/40...  Training Step: 1401...  Training loss: 1.5342...  0.5163 sec/batch\n",
      "Epoch: 36/40...  Training Step: 1402...  Training loss: 1.4933...  0.5026 sec/batch\n",
      "Epoch: 36/40...  Training Step: 1403...  Training loss: 1.4983...  0.5180 sec/batch\n",
      "Epoch: 36/40...  Training Step: 1404...  Training loss: 1.4935...  0.5180 sec/batch\n",
      "Epoch: 36/40...  Training Step: 1405...  Training loss: 1.4972...  0.5179 sec/batch\n",
      "Epoch: 36/40...  Training Step: 1406...  Training loss: 1.4822...  0.5188 sec/batch\n",
      "Epoch: 36/40...  Training Step: 1407...  Training loss: 1.4474...  0.5126 sec/batch\n",
      "Epoch: 36/40...  Training Step: 1408...  Training loss: 1.4582...  0.5090 sec/batch\n",
      "Epoch: 36/40...  Training Step: 1409...  Training loss: 1.4683...  0.5096 sec/batch\n",
      "Epoch: 36/40...  Training Step: 1410...  Training loss: 1.4663...  0.5102 sec/batch\n",
      "Epoch: 36/40...  Training Step: 1411...  Training loss: 1.4948...  0.5243 sec/batch\n",
      "Epoch: 36/40...  Training Step: 1412...  Training loss: 1.4869...  0.5113 sec/batch\n",
      "Epoch: 36/40...  Training Step: 1413...  Training loss: 1.4677...  0.5051 sec/batch\n",
      "Epoch: 36/40...  Training Step: 1414...  Training loss: 1.4970...  0.5097 sec/batch\n",
      "Epoch: 36/40...  Training Step: 1415...  Training loss: 1.5097...  0.5215 sec/batch\n",
      "Epoch: 36/40...  Training Step: 1416...  Training loss: 1.4931...  0.5129 sec/batch\n",
      "Epoch: 36/40...  Training Step: 1417...  Training loss: 1.4579...  0.5198 sec/batch\n",
      "Epoch: 36/40...  Training Step: 1418...  Training loss: 1.4796...  0.5199 sec/batch\n",
      "Epoch: 36/40...  Training Step: 1419...  Training loss: 1.4689...  0.5207 sec/batch\n",
      "Epoch: 36/40...  Training Step: 1420...  Training loss: 1.4717...  0.5179 sec/batch\n",
      "Epoch: 36/40...  Training Step: 1421...  Training loss: 1.4800...  0.5229 sec/batch\n",
      "Epoch: 36/40...  Training Step: 1422...  Training loss: 1.4890...  0.5169 sec/batch\n",
      "Epoch: 36/40...  Training Step: 1423...  Training loss: 1.4706...  0.5040 sec/batch\n",
      "Epoch: 36/40...  Training Step: 1424...  Training loss: 1.4749...  0.5195 sec/batch\n",
      "Epoch: 36/40...  Training Step: 1425...  Training loss: 1.4432...  0.5176 sec/batch\n",
      "Epoch: 36/40...  Training Step: 1426...  Training loss: 1.4756...  0.5217 sec/batch\n",
      "Epoch: 36/40...  Training Step: 1427...  Training loss: 1.4559...  0.5202 sec/batch\n",
      "Epoch: 36/40...  Training Step: 1428...  Training loss: 1.4684...  0.5151 sec/batch\n",
      "Epoch: 36/40...  Training Step: 1429...  Training loss: 1.4797...  0.5214 sec/batch\n",
      "Epoch: 36/40...  Training Step: 1430...  Training loss: 1.4813...  0.5192 sec/batch\n",
      "Epoch: 36/40...  Training Step: 1431...  Training loss: 1.4743...  0.5100 sec/batch\n",
      "Epoch: 36/40...  Training Step: 1432...  Training loss: 1.4706...  0.5305 sec/batch\n",
      "Epoch: 36/40...  Training Step: 1433...  Training loss: 1.4940...  0.5151 sec/batch\n",
      "Epoch: 36/40...  Training Step: 1434...  Training loss: 1.5080...  0.5205 sec/batch\n",
      "Epoch: 36/40...  Training Step: 1435...  Training loss: 1.4362...  0.5246 sec/batch\n",
      "Epoch: 36/40...  Training Step: 1436...  Training loss: 1.4915...  0.5090 sec/batch\n",
      "Epoch: 36/40...  Training Step: 1437...  Training loss: 1.4654...  0.5017 sec/batch\n",
      "Epoch: 36/40...  Training Step: 1438...  Training loss: 1.5187...  0.5183 sec/batch\n",
      "Epoch: 36/40...  Training Step: 1439...  Training loss: 1.4909...  0.5271 sec/batch\n",
      "Epoch: 36/40...  Training Step: 1440...  Training loss: 1.4571...  0.5176 sec/batch\n",
      "Epoch: 37/40...  Training Step: 1441...  Training loss: 1.5189...  0.5148 sec/batch\n",
      "Epoch: 37/40...  Training Step: 1442...  Training loss: 1.4965...  0.5237 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37/40...  Training Step: 1443...  Training loss: 1.4928...  0.5164 sec/batch\n",
      "Epoch: 37/40...  Training Step: 1444...  Training loss: 1.4772...  0.5129 sec/batch\n",
      "Epoch: 37/40...  Training Step: 1445...  Training loss: 1.4765...  0.5202 sec/batch\n",
      "Epoch: 37/40...  Training Step: 1446...  Training loss: 1.4683...  0.5140 sec/batch\n",
      "Epoch: 37/40...  Training Step: 1447...  Training loss: 1.4483...  0.5072 sec/batch\n",
      "Epoch: 37/40...  Training Step: 1448...  Training loss: 1.4552...  0.5231 sec/batch\n",
      "Epoch: 37/40...  Training Step: 1449...  Training loss: 1.4572...  0.5239 sec/batch\n",
      "Epoch: 37/40...  Training Step: 1450...  Training loss: 1.4607...  0.5153 sec/batch\n",
      "Epoch: 37/40...  Training Step: 1451...  Training loss: 1.4915...  0.5121 sec/batch\n",
      "Epoch: 37/40...  Training Step: 1452...  Training loss: 1.4781...  0.5121 sec/batch\n",
      "Epoch: 37/40...  Training Step: 1453...  Training loss: 1.4622...  0.5138 sec/batch\n",
      "Epoch: 37/40...  Training Step: 1454...  Training loss: 1.4852...  0.5214 sec/batch\n",
      "Epoch: 37/40...  Training Step: 1455...  Training loss: 1.5101...  0.5052 sec/batch\n",
      "Epoch: 37/40...  Training Step: 1456...  Training loss: 1.4909...  0.5178 sec/batch\n",
      "Epoch: 37/40...  Training Step: 1457...  Training loss: 1.4503...  0.5255 sec/batch\n",
      "Epoch: 37/40...  Training Step: 1458...  Training loss: 1.4698...  0.5246 sec/batch\n",
      "Epoch: 37/40...  Training Step: 1459...  Training loss: 1.4557...  0.5133 sec/batch\n",
      "Epoch: 37/40...  Training Step: 1460...  Training loss: 1.4608...  0.5126 sec/batch\n",
      "Epoch: 37/40...  Training Step: 1461...  Training loss: 1.4714...  0.5188 sec/batch\n",
      "Epoch: 37/40...  Training Step: 1462...  Training loss: 1.4809...  0.5144 sec/batch\n",
      "Epoch: 37/40...  Training Step: 1463...  Training loss: 1.4640...  0.5164 sec/batch\n",
      "Epoch: 37/40...  Training Step: 1464...  Training loss: 1.4649...  0.5100 sec/batch\n",
      "Epoch: 37/40...  Training Step: 1465...  Training loss: 1.4313...  0.5227 sec/batch\n",
      "Epoch: 37/40...  Training Step: 1466...  Training loss: 1.4620...  0.5118 sec/batch\n",
      "Epoch: 37/40...  Training Step: 1467...  Training loss: 1.4540...  0.5091 sec/batch\n",
      "Epoch: 37/40...  Training Step: 1468...  Training loss: 1.4621...  0.5222 sec/batch\n",
      "Epoch: 37/40...  Training Step: 1469...  Training loss: 1.4681...  0.5163 sec/batch\n",
      "Epoch: 37/40...  Training Step: 1470...  Training loss: 1.4643...  0.5188 sec/batch\n",
      "Epoch: 37/40...  Training Step: 1471...  Training loss: 1.4635...  0.5078 sec/batch\n",
      "Epoch: 37/40...  Training Step: 1472...  Training loss: 1.4620...  0.5199 sec/batch\n",
      "Epoch: 37/40...  Training Step: 1473...  Training loss: 1.4794...  0.5307 sec/batch\n",
      "Epoch: 37/40...  Training Step: 1474...  Training loss: 1.4842...  0.5129 sec/batch\n",
      "Epoch: 37/40...  Training Step: 1475...  Training loss: 1.4325...  0.5191 sec/batch\n",
      "Epoch: 37/40...  Training Step: 1476...  Training loss: 1.4836...  0.5213 sec/batch\n",
      "Epoch: 37/40...  Training Step: 1477...  Training loss: 1.4468...  0.5172 sec/batch\n",
      "Epoch: 37/40...  Training Step: 1478...  Training loss: 1.5034...  0.5136 sec/batch\n",
      "Epoch: 37/40...  Training Step: 1479...  Training loss: 1.4845...  0.5139 sec/batch\n",
      "Epoch: 37/40...  Training Step: 1480...  Training loss: 1.4506...  0.5132 sec/batch\n",
      "Epoch: 38/40...  Training Step: 1481...  Training loss: 1.5054...  0.5186 sec/batch\n",
      "Epoch: 38/40...  Training Step: 1482...  Training loss: 1.4847...  0.5199 sec/batch\n",
      "Epoch: 38/40...  Training Step: 1483...  Training loss: 1.4877...  0.5236 sec/batch\n",
      "Epoch: 38/40...  Training Step: 1484...  Training loss: 1.4708...  0.5124 sec/batch\n",
      "Epoch: 38/40...  Training Step: 1485...  Training loss: 1.4718...  0.5180 sec/batch\n",
      "Epoch: 38/40...  Training Step: 1486...  Training loss: 1.4686...  0.5186 sec/batch\n",
      "Epoch: 38/40...  Training Step: 1487...  Training loss: 1.4265...  0.5155 sec/batch\n",
      "Epoch: 38/40...  Training Step: 1488...  Training loss: 1.4404...  0.5256 sec/batch\n",
      "Epoch: 38/40...  Training Step: 1489...  Training loss: 1.4636...  0.5118 sec/batch\n",
      "Epoch: 38/40...  Training Step: 1490...  Training loss: 1.4503...  0.5169 sec/batch\n",
      "Epoch: 38/40...  Training Step: 1491...  Training loss: 1.4743...  0.5056 sec/batch\n",
      "Epoch: 38/40...  Training Step: 1492...  Training loss: 1.4618...  0.5172 sec/batch\n",
      "Epoch: 38/40...  Training Step: 1493...  Training loss: 1.4431...  0.5190 sec/batch\n",
      "Epoch: 38/40...  Training Step: 1494...  Training loss: 1.4859...  0.5130 sec/batch\n",
      "Epoch: 38/40...  Training Step: 1495...  Training loss: 1.4942...  0.5142 sec/batch\n",
      "Epoch: 38/40...  Training Step: 1496...  Training loss: 1.4788...  0.5107 sec/batch\n",
      "Epoch: 38/40...  Training Step: 1497...  Training loss: 1.4415...  0.5220 sec/batch\n",
      "Epoch: 38/40...  Training Step: 1498...  Training loss: 1.4626...  0.5254 sec/batch\n",
      "Epoch: 38/40...  Training Step: 1499...  Training loss: 1.4507...  0.5173 sec/batch\n",
      "Epoch: 38/40...  Training Step: 1500...  Training loss: 1.4490...  0.5188 sec/batch\n",
      "Epoch: 38/40...  Training Step: 1501...  Training loss: 1.4552...  0.5187 sec/batch\n",
      "Epoch: 38/40...  Training Step: 1502...  Training loss: 1.4722...  0.5092 sec/batch\n",
      "Epoch: 38/40...  Training Step: 1503...  Training loss: 1.4533...  0.5063 sec/batch\n",
      "Epoch: 38/40...  Training Step: 1504...  Training loss: 1.4558...  0.5141 sec/batch\n",
      "Epoch: 38/40...  Training Step: 1505...  Training loss: 1.4229...  0.5194 sec/batch\n",
      "Epoch: 38/40...  Training Step: 1506...  Training loss: 1.4487...  0.5185 sec/batch\n",
      "Epoch: 38/40...  Training Step: 1507...  Training loss: 1.4399...  0.5120 sec/batch\n",
      "Epoch: 38/40...  Training Step: 1508...  Training loss: 1.4429...  0.5098 sec/batch\n",
      "Epoch: 38/40...  Training Step: 1509...  Training loss: 1.4631...  0.5258 sec/batch\n",
      "Epoch: 38/40...  Training Step: 1510...  Training loss: 1.4538...  0.5206 sec/batch\n",
      "Epoch: 38/40...  Training Step: 1511...  Training loss: 1.4509...  0.5062 sec/batch\n",
      "Epoch: 38/40...  Training Step: 1512...  Training loss: 1.4653...  0.5127 sec/batch\n",
      "Epoch: 38/40...  Training Step: 1513...  Training loss: 1.4654...  0.5093 sec/batch\n",
      "Epoch: 38/40...  Training Step: 1514...  Training loss: 1.4875...  0.5124 sec/batch\n",
      "Epoch: 38/40...  Training Step: 1515...  Training loss: 1.4198...  0.5097 sec/batch\n",
      "Epoch: 38/40...  Training Step: 1516...  Training loss: 1.4696...  0.5248 sec/batch\n",
      "Epoch: 38/40...  Training Step: 1517...  Training loss: 1.4435...  0.5242 sec/batch\n",
      "Epoch: 38/40...  Training Step: 1518...  Training loss: 1.4982...  0.5245 sec/batch\n",
      "Epoch: 38/40...  Training Step: 1519...  Training loss: 1.4732...  0.5097 sec/batch\n",
      "Epoch: 38/40...  Training Step: 1520...  Training loss: 1.4361...  0.5262 sec/batch\n",
      "Epoch: 39/40...  Training Step: 1521...  Training loss: 1.5017...  0.5147 sec/batch\n",
      "Epoch: 39/40...  Training Step: 1522...  Training loss: 1.4522...  0.5125 sec/batch\n",
      "Epoch: 39/40...  Training Step: 1523...  Training loss: 1.4673...  0.5176 sec/batch\n",
      "Epoch: 39/40...  Training Step: 1524...  Training loss: 1.4503...  0.5042 sec/batch\n",
      "Epoch: 39/40...  Training Step: 1525...  Training loss: 1.4675...  0.5100 sec/batch\n",
      "Epoch: 39/40...  Training Step: 1526...  Training loss: 1.4518...  0.5111 sec/batch\n",
      "Epoch: 39/40...  Training Step: 1527...  Training loss: 1.4116...  0.5268 sec/batch\n",
      "Epoch: 39/40...  Training Step: 1528...  Training loss: 1.4310...  0.5115 sec/batch\n",
      "Epoch: 39/40...  Training Step: 1529...  Training loss: 1.4436...  0.5202 sec/batch\n",
      "Epoch: 39/40...  Training Step: 1530...  Training loss: 1.4416...  0.5223 sec/batch\n",
      "Epoch: 39/40...  Training Step: 1531...  Training loss: 1.4647...  0.5155 sec/batch\n",
      "Epoch: 39/40...  Training Step: 1532...  Training loss: 1.4478...  0.5200 sec/batch\n",
      "Epoch: 39/40...  Training Step: 1533...  Training loss: 1.4364...  0.5211 sec/batch\n",
      "Epoch: 39/40...  Training Step: 1534...  Training loss: 1.4644...  0.5036 sec/batch\n",
      "Epoch: 39/40...  Training Step: 1535...  Training loss: 1.4773...  0.5087 sec/batch\n",
      "Epoch: 39/40...  Training Step: 1536...  Training loss: 1.4696...  0.5109 sec/batch\n",
      "Epoch: 39/40...  Training Step: 1537...  Training loss: 1.4282...  0.5246 sec/batch\n",
      "Epoch: 39/40...  Training Step: 1538...  Training loss: 1.4505...  0.5239 sec/batch\n",
      "Epoch: 39/40...  Training Step: 1539...  Training loss: 1.4256...  0.5190 sec/batch\n",
      "Epoch: 39/40...  Training Step: 1540...  Training loss: 1.4282...  0.5217 sec/batch\n",
      "Epoch: 39/40...  Training Step: 1541...  Training loss: 1.4379...  0.5201 sec/batch\n",
      "Epoch: 39/40...  Training Step: 1542...  Training loss: 1.4528...  0.5100 sec/batch\n",
      "Epoch: 39/40...  Training Step: 1543...  Training loss: 1.4456...  0.5094 sec/batch\n",
      "Epoch: 39/40...  Training Step: 1544...  Training loss: 1.4378...  0.5074 sec/batch\n",
      "Epoch: 39/40...  Training Step: 1545...  Training loss: 1.4064...  0.5118 sec/batch\n",
      "Epoch: 39/40...  Training Step: 1546...  Training loss: 1.4364...  0.5207 sec/batch\n",
      "Epoch: 39/40...  Training Step: 1547...  Training loss: 1.4280...  0.5241 sec/batch\n",
      "Epoch: 39/40...  Training Step: 1548...  Training loss: 1.4331...  0.5179 sec/batch\n",
      "Epoch: 39/40...  Training Step: 1549...  Training loss: 1.4494...  0.5202 sec/batch\n",
      "Epoch: 39/40...  Training Step: 1550...  Training loss: 1.4378...  0.5228 sec/batch\n",
      "Epoch: 39/40...  Training Step: 1551...  Training loss: 1.4438...  0.5142 sec/batch\n",
      "Epoch: 39/40...  Training Step: 1552...  Training loss: 1.4463...  0.5146 sec/batch\n",
      "Epoch: 39/40...  Training Step: 1553...  Training loss: 1.4486...  0.5149 sec/batch\n",
      "Epoch: 39/40...  Training Step: 1554...  Training loss: 1.4627...  0.5060 sec/batch\n",
      "Epoch: 39/40...  Training Step: 1555...  Training loss: 1.4015...  0.5231 sec/batch\n",
      "Epoch: 39/40...  Training Step: 1556...  Training loss: 1.4533...  0.5201 sec/batch\n",
      "Epoch: 39/40...  Training Step: 1557...  Training loss: 1.4276...  0.5179 sec/batch\n",
      "Epoch: 39/40...  Training Step: 1558...  Training loss: 1.4818...  0.5197 sec/batch\n",
      "Epoch: 39/40...  Training Step: 1559...  Training loss: 1.4632...  0.5217 sec/batch\n",
      "Epoch: 39/40...  Training Step: 1560...  Training loss: 1.4230...  0.5197 sec/batch\n",
      "Epoch: 40/40...  Training Step: 1561...  Training loss: 1.4775...  0.5117 sec/batch\n",
      "Epoch: 40/40...  Training Step: 1562...  Training loss: 1.4480...  0.5115 sec/batch\n",
      "Epoch: 40/40...  Training Step: 1563...  Training loss: 1.4478...  0.5178 sec/batch\n",
      "Epoch: 40/40...  Training Step: 1564...  Training loss: 1.4372...  0.5064 sec/batch\n",
      "Epoch: 40/40...  Training Step: 1565...  Training loss: 1.4529...  0.5039 sec/batch\n",
      "Epoch: 40/40...  Training Step: 1566...  Training loss: 1.4428...  0.5185 sec/batch\n",
      "Epoch: 40/40...  Training Step: 1567...  Training loss: 1.4039...  0.5144 sec/batch\n",
      "Epoch: 40/40...  Training Step: 1568...  Training loss: 1.4115...  0.5110 sec/batch\n",
      "Epoch: 40/40...  Training Step: 1569...  Training loss: 1.4272...  0.5027 sec/batch\n",
      "Epoch: 40/40...  Training Step: 1570...  Training loss: 1.4244...  0.5197 sec/batch\n",
      "Epoch: 40/40...  Training Step: 1571...  Training loss: 1.4469...  0.5096 sec/batch\n",
      "Epoch: 40/40...  Training Step: 1572...  Training loss: 1.4384...  0.5164 sec/batch\n",
      "Epoch: 40/40...  Training Step: 1573...  Training loss: 1.4216...  0.5199 sec/batch\n",
      "Epoch: 40/40...  Training Step: 1574...  Training loss: 1.4523...  0.5253 sec/batch\n",
      "Epoch: 40/40...  Training Step: 1575...  Training loss: 1.4686...  0.5121 sec/batch\n",
      "Epoch: 40/40...  Training Step: 1576...  Training loss: 1.4557...  0.5132 sec/batch\n",
      "Epoch: 40/40...  Training Step: 1577...  Training loss: 1.4160...  0.5010 sec/batch\n",
      "Epoch: 40/40...  Training Step: 1578...  Training loss: 1.4375...  0.5077 sec/batch\n",
      "Epoch: 40/40...  Training Step: 1579...  Training loss: 1.4110...  0.5189 sec/batch\n",
      "Epoch: 40/40...  Training Step: 1580...  Training loss: 1.4236...  0.5193 sec/batch\n",
      "Epoch: 40/40...  Training Step: 1581...  Training loss: 1.4293...  0.5221 sec/batch\n",
      "Epoch: 40/40...  Training Step: 1582...  Training loss: 1.4364...  0.5219 sec/batch\n",
      "Epoch: 40/40...  Training Step: 1583...  Training loss: 1.4298...  0.5197 sec/batch\n",
      "Epoch: 40/40...  Training Step: 1584...  Training loss: 1.4408...  0.5097 sec/batch\n",
      "Epoch: 40/40...  Training Step: 1585...  Training loss: 1.3975...  0.5058 sec/batch\n",
      "Epoch: 40/40...  Training Step: 1586...  Training loss: 1.4250...  0.5203 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40/40...  Training Step: 1587...  Training loss: 1.4046...  0.5208 sec/batch\n",
      "Epoch: 40/40...  Training Step: 1588...  Training loss: 1.4209...  0.5262 sec/batch\n",
      "Epoch: 40/40...  Training Step: 1589...  Training loss: 1.4331...  0.5166 sec/batch\n",
      "Epoch: 40/40...  Training Step: 1590...  Training loss: 1.4327...  0.5114 sec/batch\n",
      "Epoch: 40/40...  Training Step: 1591...  Training loss: 1.4288...  0.5185 sec/batch\n",
      "Epoch: 40/40...  Training Step: 1592...  Training loss: 1.4311...  0.5094 sec/batch\n",
      "Epoch: 40/40...  Training Step: 1593...  Training loss: 1.4442...  0.5072 sec/batch\n",
      "Epoch: 40/40...  Training Step: 1594...  Training loss: 1.4552...  0.5202 sec/batch\n",
      "Epoch: 40/40...  Training Step: 1595...  Training loss: 1.4035...  0.5247 sec/batch\n",
      "Epoch: 40/40...  Training Step: 1596...  Training loss: 1.4494...  0.5230 sec/batch\n",
      "Epoch: 40/40...  Training Step: 1597...  Training loss: 1.4226...  0.5237 sec/batch\n",
      "Epoch: 40/40...  Training Step: 1598...  Training loss: 1.4754...  0.5211 sec/batch\n",
      "Epoch: 40/40...  Training Step: 1599...  Training loss: 1.4472...  0.5223 sec/batch\n",
      "Epoch: 40/40...  Training Step: 1600...  Training loss: 1.4158...  0.5077 sec/batch\n"
     ]
    }
   ],
   "source": [
    "train(encoded, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "vkTK4B-lgmBJ"
   },
   "outputs": [],
   "source": [
    "files.download(tf.train.latest_checkpoint('checkpoints') + '.meta')\n",
    "files.download(tf.train.latest_checkpoint('checkpoints') + '.index')\n",
    "files.download(tf.train.latest_checkpoint('checkpoints') + '.data-00000-of-00001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5083,
     "status": "ok",
     "timestamp": 1526165360701,
     "user": {
      "displayName": "Mattia Mancassola",
      "photoUrl": "//lh3.googleusercontent.com/-lyRwyRCbSzY/AAAAAAAAAAI/AAAAAAAAUYQ/zHzRCLhMZRs/s50-c-k-no/photo.jpg",
      "userId": "116324510352588137280"
     },
     "user_tz": -120
    },
    "id": "l2735KEy5f4D",
    "outputId": "e9c4e37f-0aff-40d1-d7f3-937cdd3eefc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i1600_l512.ckpt\n",
      "King there oneself alone\n",
      "when the standard and metaphysics. They have always the\n",
      "stated of the can to see in the present morality which they will\n",
      "be a despection and the subjution of the concepsion, an indiversally\n",
      "individual thers and instracted in the sullimination of the world with\n",
      "a philosopher and the present their power. All their sours or also\n",
      "there are self-decousive than to the master, and alse themselve\n",
      "with he must both among teder our probect of heresing and subject of\n",
      "their action and such a closy strong explanation, in order to the past\n",
      "of construttry at a southern tratsed of a had to all thinh and traunds of\n",
      "the soul andievy as and the constitutious, the strungefle still same,\n",
      "the succersful instinct of the marbelian, which it is the some\n",
      "ordinary think in a conception. The popenting tenses of things\n",
      "are soul. Their it was the powerfol, and always the preyers of has\n",
      "to bad consition on an extent of all truth to the moralizical, and\n",
      "ortaining itself and always allow, to the spa\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, vocab, vocab_to_int, int_to_vocab, 1000, lstm_size, len(vocab), seed=\"King \")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "lstm_text_generator.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
